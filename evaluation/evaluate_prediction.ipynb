{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a3deb2ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved summary to Aug_8/evaluation_summary.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Environment</th>\n",
       "      <th>TrainSamples</th>\n",
       "      <th>EncodeDim</th>\n",
       "      <th>UseCovLoss</th>\n",
       "      <th>UseControlLoss</th>\n",
       "      <th>WeightedError</th>\n",
       "      <th>NormalizedCovLoss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Franka</td>\n",
       "      <td>60000</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.453208e-06</td>\n",
       "      <td>4.605299e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Franka</td>\n",
       "      <td>60000</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3.664425e-06</td>\n",
       "      <td>1.444889e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Franka</td>\n",
       "      <td>60000</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.969726e-06</td>\n",
       "      <td>1.742244e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Franka</td>\n",
       "      <td>60000</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4.469529e-06</td>\n",
       "      <td>1.128509e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Franka</td>\n",
       "      <td>60000</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.175875e-07</td>\n",
       "      <td>4.222307e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Franka</td>\n",
       "      <td>60000</td>\n",
       "      <td>34</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.065051e-07</td>\n",
       "      <td>9.452341e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Franka</td>\n",
       "      <td>60000</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6.617872e-08</td>\n",
       "      <td>4.989505e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Franka</td>\n",
       "      <td>60000</td>\n",
       "      <td>34</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.028723e-07</td>\n",
       "      <td>8.238921e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Franka</td>\n",
       "      <td>60000</td>\n",
       "      <td>68</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.263802e-08</td>\n",
       "      <td>2.005329e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Franka</td>\n",
       "      <td>60000</td>\n",
       "      <td>68</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2.486477e-08</td>\n",
       "      <td>3.350522e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Franka</td>\n",
       "      <td>60000</td>\n",
       "      <td>68</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.404339e-08</td>\n",
       "      <td>1.329919e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Franka</td>\n",
       "      <td>60000</td>\n",
       "      <td>68</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2.718322e-08</td>\n",
       "      <td>3.340875e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Franka</td>\n",
       "      <td>60000</td>\n",
       "      <td>136</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.390556e-08</td>\n",
       "      <td>1.401901e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Franka</td>\n",
       "      <td>60000</td>\n",
       "      <td>136</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2.689134e-08</td>\n",
       "      <td>1.808323e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Franka</td>\n",
       "      <td>60000</td>\n",
       "      <td>136</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.514716e-08</td>\n",
       "      <td>9.317557e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Franka</td>\n",
       "      <td>60000</td>\n",
       "      <td>136</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2.805794e-08</td>\n",
       "      <td>1.823374e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Franka</td>\n",
       "      <td>60000</td>\n",
       "      <td>272</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.231682e-08</td>\n",
       "      <td>1.242050e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Franka</td>\n",
       "      <td>60000</td>\n",
       "      <td>272</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2.296667e-08</td>\n",
       "      <td>9.826267e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Franka</td>\n",
       "      <td>60000</td>\n",
       "      <td>272</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.211244e-08</td>\n",
       "      <td>8.091945e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Franka</td>\n",
       "      <td>60000</td>\n",
       "      <td>272</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2.312353e-08</td>\n",
       "      <td>9.905710e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Franka</td>\n",
       "      <td>1000</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.443345e-06</td>\n",
       "      <td>4.745956e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Franka</td>\n",
       "      <td>1000</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3.640162e-06</td>\n",
       "      <td>1.526288e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Franka</td>\n",
       "      <td>1000</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4.377587e-06</td>\n",
       "      <td>1.734539e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Franka</td>\n",
       "      <td>1000</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4.553154e-06</td>\n",
       "      <td>1.229345e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Franka</td>\n",
       "      <td>1000</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.249042e-07</td>\n",
       "      <td>4.481220e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Franka</td>\n",
       "      <td>1000</td>\n",
       "      <td>34</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.470441e-07</td>\n",
       "      <td>1.037312e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Franka</td>\n",
       "      <td>1000</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7.326369e-08</td>\n",
       "      <td>4.608912e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Franka</td>\n",
       "      <td>1000</td>\n",
       "      <td>34</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>8.686391e-08</td>\n",
       "      <td>8.725464e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Franka</td>\n",
       "      <td>1000</td>\n",
       "      <td>68</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.373511e-08</td>\n",
       "      <td>1.988457e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Franka</td>\n",
       "      <td>1000</td>\n",
       "      <td>68</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3.556734e-08</td>\n",
       "      <td>3.419467e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Franka</td>\n",
       "      <td>1000</td>\n",
       "      <td>68</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.789149e-08</td>\n",
       "      <td>1.317386e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Franka</td>\n",
       "      <td>1000</td>\n",
       "      <td>68</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3.080560e-08</td>\n",
       "      <td>3.396823e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Franka</td>\n",
       "      <td>1000</td>\n",
       "      <td>136</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.301416e-08</td>\n",
       "      <td>1.420860e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Franka</td>\n",
       "      <td>1000</td>\n",
       "      <td>136</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3.811134e-08</td>\n",
       "      <td>1.868326e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Franka</td>\n",
       "      <td>1000</td>\n",
       "      <td>136</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.988441e-08</td>\n",
       "      <td>9.463672e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Franka</td>\n",
       "      <td>1000</td>\n",
       "      <td>136</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3.312762e-08</td>\n",
       "      <td>1.838830e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Franka</td>\n",
       "      <td>1000</td>\n",
       "      <td>272</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.884599e-08</td>\n",
       "      <td>1.198400e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Franka</td>\n",
       "      <td>1000</td>\n",
       "      <td>272</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3.300620e-08</td>\n",
       "      <td>1.003572e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Franka</td>\n",
       "      <td>1000</td>\n",
       "      <td>272</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.560161e-08</td>\n",
       "      <td>8.376106e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>Franka</td>\n",
       "      <td>1000</td>\n",
       "      <td>272</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2.631407e-08</td>\n",
       "      <td>1.013469e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>Franka</td>\n",
       "      <td>4000</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.074453e-06</td>\n",
       "      <td>5.251632e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>Franka</td>\n",
       "      <td>16000</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.118289e-06</td>\n",
       "      <td>5.756139e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Franka</td>\n",
       "      <td>4000</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4.279743e-06</td>\n",
       "      <td>1.472713e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>Franka</td>\n",
       "      <td>16000</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4.352199e-06</td>\n",
       "      <td>1.470193e-06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Environment  TrainSamples  EncodeDim  UseCovLoss  UseControlLoss  \\\n",
       "0       Franka         60000         17           0               0   \n",
       "1       Franka         60000         17           1               0   \n",
       "2       Franka         60000         17           0               1   \n",
       "3       Franka         60000         17           1               1   \n",
       "4       Franka         60000         34           0               0   \n",
       "5       Franka         60000         34           1               0   \n",
       "6       Franka         60000         34           0               1   \n",
       "7       Franka         60000         34           1               1   \n",
       "8       Franka         60000         68           0               0   \n",
       "9       Franka         60000         68           1               0   \n",
       "10      Franka         60000         68           0               1   \n",
       "11      Franka         60000         68           1               1   \n",
       "12      Franka         60000        136           0               0   \n",
       "13      Franka         60000        136           1               0   \n",
       "14      Franka         60000        136           0               1   \n",
       "15      Franka         60000        136           1               1   \n",
       "16      Franka         60000        272           0               0   \n",
       "17      Franka         60000        272           1               0   \n",
       "18      Franka         60000        272           0               1   \n",
       "19      Franka         60000        272           1               1   \n",
       "20      Franka          1000         17           0               0   \n",
       "21      Franka          1000         17           1               0   \n",
       "22      Franka          1000         17           0               1   \n",
       "23      Franka          1000         17           1               1   \n",
       "24      Franka          1000         34           0               0   \n",
       "25      Franka          1000         34           1               0   \n",
       "26      Franka          1000         34           0               1   \n",
       "27      Franka          1000         34           1               1   \n",
       "28      Franka          1000         68           0               0   \n",
       "29      Franka          1000         68           1               0   \n",
       "30      Franka          1000         68           0               1   \n",
       "31      Franka          1000         68           1               1   \n",
       "32      Franka          1000        136           0               0   \n",
       "33      Franka          1000        136           1               0   \n",
       "34      Franka          1000        136           0               1   \n",
       "35      Franka          1000        136           1               1   \n",
       "36      Franka          1000        272           0               0   \n",
       "37      Franka          1000        272           1               0   \n",
       "38      Franka          1000        272           0               1   \n",
       "39      Franka          1000        272           1               1   \n",
       "40      Franka          4000         17           0               0   \n",
       "41      Franka         16000         17           0               0   \n",
       "42      Franka          4000         17           1               0   \n",
       "43      Franka         16000         17           1               0   \n",
       "\n",
       "    WeightedError  NormalizedCovLoss  \n",
       "0    3.453208e-06       4.605299e-05  \n",
       "1    3.664425e-06       1.444889e-06  \n",
       "2    3.969726e-06       1.742244e-04  \n",
       "3    4.469529e-06       1.128509e-06  \n",
       "4    1.175875e-07       4.222307e-05  \n",
       "5    1.065051e-07       9.452341e-07  \n",
       "6    6.617872e-08       4.989505e-05  \n",
       "7    1.028723e-07       8.238921e-07  \n",
       "8    2.263802e-08       2.005329e-05  \n",
       "9    2.486477e-08       3.350522e-07  \n",
       "10   2.404339e-08       1.329919e-05  \n",
       "11   2.718322e-08       3.340875e-07  \n",
       "12   2.390556e-08       1.401901e-05  \n",
       "13   2.689134e-08       1.808323e-07  \n",
       "14   2.514716e-08       9.317557e-06  \n",
       "15   2.805794e-08       1.823374e-07  \n",
       "16   2.231682e-08       1.242050e-05  \n",
       "17   2.296667e-08       9.826267e-08  \n",
       "18   2.211244e-08       8.091945e-06  \n",
       "19   2.312353e-08       9.905710e-08  \n",
       "20   3.443345e-06       4.745956e-05  \n",
       "21   3.640162e-06       1.526288e-06  \n",
       "22   4.377587e-06       1.734539e-04  \n",
       "23   4.553154e-06       1.229345e-06  \n",
       "24   2.249042e-07       4.481220e-05  \n",
       "25   1.470441e-07       1.037312e-06  \n",
       "26   7.326369e-08       4.608912e-05  \n",
       "27   8.686391e-08       8.725464e-07  \n",
       "28   4.373511e-08       1.988457e-05  \n",
       "29   3.556734e-08       3.419467e-07  \n",
       "30   2.789149e-08       1.317386e-05  \n",
       "31   3.080560e-08       3.396823e-07  \n",
       "32   4.301416e-08       1.420860e-05  \n",
       "33   3.811134e-08       1.868326e-07  \n",
       "34   2.988441e-08       9.463672e-06  \n",
       "35   3.312762e-08       1.838830e-07  \n",
       "36   3.884599e-08       1.198400e-05  \n",
       "37   3.300620e-08       1.003572e-07  \n",
       "38   2.560161e-08       8.376106e-06  \n",
       "39   2.631407e-08       1.013469e-07  \n",
       "40   4.074453e-06       5.251632e-05  \n",
       "41   4.118289e-06       5.756139e-05  \n",
       "42   4.279743e-06       1.472713e-06  \n",
       "43   4.352199e-06       1.470193e-06  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: Aug_8/Franka/Franka_TrainSamples_vs_Error.png\n",
      "Saved: Aug_8/Franka/Franka_EncodeDim_vs_Error.png\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import math\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "\n",
    "# =========================\n",
    "# Project setup\n",
    "# =========================\n",
    "sys.path.append(\"../scripts\")\n",
    "sys.path.append(\"../utility\")\n",
    "from network import KoopmanNet\n",
    "\n",
    "project_name = \"Aug_8\"\n",
    "gamma        = 0.8\n",
    "device       = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "ksteps       = 15\n",
    "normalize    = \"nonorm\"  # global default; overridden per env below\n",
    "\n",
    "# Encode-dim interpretation for FILTERING ONLY (loading is checkpoint-driven)\n",
    "# - \"multiplier\": encode_dims are multipliers of state_dim (old behavior)\n",
    "# - \"absolute\":   encode_dims are actual latent sizes\n",
    "encode_dim_mode = \"multiplier\"\n",
    "\n",
    "# =========================\n",
    "# Experiments to run\n",
    "# =========================\n",
    "envs        = ['Franka']#[\"DampingPendulum\",\"Franka\",\"DoublePendulum\",\"Polynomial\",\"Kinova\", \"G1\", \"Go2\"]  # add others as needed\n",
    "encode_dims = [1, 2, 4, 8, 16]      # interpreted per encode_dim_mode (for FILTERING/LABELS)\n",
    "cov_regs    = [0, 1]\n",
    "ctrl_regs   = [0, 1]\n",
    "seeds       = [17382, 76849, 20965, 84902, 51194]\n",
    "m           = 100\n",
    "\n",
    "# =========================\n",
    "# Default & per-env train sizes\n",
    "# =========================\n",
    "default_train_samples = [1000, 4000, 16000, 60000]\n",
    "\n",
    "def adjust_train_samples(env: str, samples: list[int]) -> list[int]:\n",
    "    \"\"\"\n",
    "    Scale the default train_samples depending on env so that the\n",
    "    max target is 200000 for G1 and 140000 for Go2.\n",
    "    \"\"\"\n",
    "    if env == \"G1\":\n",
    "        return [int(s * 200000 / 60000) for s in samples]\n",
    "    elif env == \"Go2\":\n",
    "        return [int(s * 140000 / 60000) for s in samples]\n",
    "    else:\n",
    "        return samples\n",
    "\n",
    "train_samples_per_env = {env: adjust_train_samples(env, default_train_samples) for env in envs}\n",
    "\n",
    "# =========================\n",
    "# Problem-specific mappings\n",
    "# =========================\n",
    "u_dim_map = {\n",
    "    \"Franka\": 7,\n",
    "    \"DoublePendulum\": 2,\n",
    "    \"DampingPendulum\": 1,\n",
    "    \"G1\": 23,\n",
    "    \"Go2\": 12,\n",
    "    \"Kinova\": 7,\n",
    "}\n",
    "\n",
    "# Override normalization per env\n",
    "normalize_override = {\"G1\": \"norm\", \"Go2\": \"norm\"}\n",
    "\n",
    "def _normalize_used_for(env: str, normalize_default: str) -> str:\n",
    "    return normalize_override.get(env, normalize_default)\n",
    "\n",
    "# =========================\n",
    "# Dataset path finder\n",
    "# =========================\n",
    "def find_dataset_path(env: str,\n",
    "                      normalize_default: str,\n",
    "                      ksteps_val: int,\n",
    "                      m_val: int,\n",
    "                      candidate_Ntrains: list[int] | None = None) -> str | None:\n",
    "    \"\"\"\n",
    "    Locate a dataset file for the given env/normalize/ksteps.\n",
    "    Tries (1) candidate_Ntrains from log, (2) env-specific defaults (adjusted),\n",
    "         (3) legacy 60000, then glob.\n",
    "    \"\"\"\n",
    "    norm = _normalize_used_for(env, normalize_default)\n",
    "    tried = []\n",
    "    if candidate_Ntrains:\n",
    "        tried.extend(candidate_Ntrains)\n",
    "    tried.extend(train_samples_per_env.get(env, default_train_samples))\n",
    "    tried.append(60000)  # legacy fallback\n",
    "\n",
    "    # dedupe preserving order\n",
    "    seen = set()\n",
    "    Ntrain_candidates = [x for x in tried if not (x in seen or seen.add(x))]\n",
    "\n",
    "    # exact attempts\n",
    "    for N in Ntrain_candidates:\n",
    "        if env == \"Polynomial\":\n",
    "            path = os.path.join(\n",
    "                \"..\", \"data\", \"datasets\",\n",
    "                f\"dataset_{env}_{norm}_m_{m_val}_Ktrain_{N}_Kval_20000_Ktest_20000_Ksteps_{ksteps_val}.pt\"\n",
    "            )\n",
    "        else:\n",
    "            path = os.path.join(\n",
    "                \"..\", \"data\", \"datasets\",\n",
    "                f\"dataset_{env}_{norm}_Ktrain_{N}_Kval_20000_Ktest_20000_Ksteps_{ksteps_val}.pt\"\n",
    "            )\n",
    "        if os.path.exists(path):\n",
    "            return path\n",
    "\n",
    "    # glob fallback (handles naming drift)\n",
    "    base_dir = os.path.join(\"..\", \"data\", \"datasets\")\n",
    "    if env == \"Polynomial\":\n",
    "        pattern = f\"dataset_{env}_{norm}_m_{m_val}_Ktrain_*_Kval_*_Ktest_*_Ksteps_{ksteps_val}.pt\"\n",
    "    else:\n",
    "        pattern = f\"dataset_{env}_{norm}_Ktrain_*_Kval_*_Ktest_*_Ksteps_{ksteps_val}.pt\"\n",
    "    matches = sorted(glob.glob(os.path.join(base_dir, pattern)))\n",
    "    return matches[0] if matches else None\n",
    "\n",
    "# =========================\n",
    "# Helpers\n",
    "# =========================\n",
    "def env_has_control(env: str) -> bool:\n",
    "    \"\"\"Return True if the environment has a control input (u_dim > 0).\"\"\"\n",
    "    return (u_dim_map.get(env, 0) or 0) > 0\n",
    "\n",
    "def _mode_matches(row_mode: str, selected_mode: str) -> bool:\n",
    "    if row_mode is None or row_mode == \"\" or pd.isna(row_mode):\n",
    "        return True\n",
    "    row_mode = str(row_mode).strip().lower()\n",
    "    selected_mode = selected_mode.strip().lower()\n",
    "    if selected_mode == \"multiplier\":\n",
    "        return row_mode in (\"times_input_dim\", \"per_state\")\n",
    "    elif selected_mode == \"absolute\":\n",
    "        return row_mode == \"absolute\"\n",
    "    return True\n",
    "\n",
    "def evaluate_model(model, data, u_dim, gamma, state_dim, device):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        steps = data.shape[0]\n",
    "        if u_dim is None or u_dim == 0:\n",
    "            X = model.encode(data[0].to(device))\n",
    "        else:\n",
    "            X = model.encode(data[0, :, u_dim:].to(device))\n",
    "        encoded_initial = X[:, state_dim:]\n",
    "\n",
    "        weighted_loss = 0.0\n",
    "        beta, beta_sum = 1.0, 0.0\n",
    "\n",
    "        for i in range(steps - 1):\n",
    "            if u_dim is None or u_dim == 0:\n",
    "                X      = model.forward(X, None)\n",
    "                target = data[i + 1].to(device)\n",
    "            else:\n",
    "                X      = model.forward(X, data[i, :, :u_dim].to(device))\n",
    "                target = data[i + 1, :, u_dim:].to(device)\n",
    "\n",
    "            error = nn.MSELoss()(X[:, :state_dim], target)\n",
    "            weighted_loss += beta * error\n",
    "            beta_sum      += beta\n",
    "            beta         *= gamma\n",
    "\n",
    "        weighted_loss /= beta_sum\n",
    "\n",
    "        # covariance on initial encoding (for reference)\n",
    "        z           = encoded_initial\n",
    "        z_centered  = z - z.mean(dim=0, keepdim=True)\n",
    "        cov_matrix  = (z_centered.t() @ z_centered) / (z_centered.size(0) - 1)\n",
    "        off_diag    = cov_matrix - torch.diag(torch.diag(cov_matrix))\n",
    "        cov_loss    = torch.norm(off_diag, p='fro') ** 2\n",
    "        encode_dim  = X.shape[1] - state_dim\n",
    "        normalized_cov_loss = (cov_loss.item() / (encode_dim * (encode_dim - 1))\n",
    "                               if encode_dim > 1 else cov_loss.item())\n",
    "\n",
    "    return float(weighted_loss.item()), float(normalized_cov_loss)\n",
    "\n",
    "def build_model_from_checkpoint(chkpt, state_dim, u_dim_hint, device):\n",
    "    \"\"\"\n",
    "    Build a KoopmanNet that exactly matches the checkpoint shapes.\n",
    "    Returns: model, u_dim_eval, enc_dim_actual, trained_Nkoop\n",
    "    \"\"\"\n",
    "    layers = chkpt[\"layer\"]\n",
    "    sd     = chkpt[\"model\"]\n",
    "\n",
    "    # Infer trained Nkoop from lA\n",
    "    if \"lA.weight\" not in sd or sd[\"lA.weight\"].ndim != 2 or sd[\"lA.weight\"].shape[0] != sd[\"lA.weight\"].shape[1]:\n",
    "        fallback = [(k, p) for k, p in sd.items() if k.endswith(\"lA.weight\") and p.ndim == 2 and p.shape[0] == p.shape[1]]\n",
    "        if not fallback:\n",
    "            raise RuntimeError(\"Cannot infer Nkoop: missing or non-square lA.weight in checkpoint.\")\n",
    "        trained_Nkoop = fallback[0][1].shape[0]\n",
    "    else:\n",
    "        trained_Nkoop = sd[\"lA.weight\"].shape[0]\n",
    "\n",
    "    # Infer u_dim from lB if present; otherwise fall back to mapping\n",
    "    if \"lB.weight\" in sd and sd[\"lB.weight\"].ndim == 2:\n",
    "        trained_u_dim = sd[\"lB.weight\"].shape[1]\n",
    "    else:\n",
    "        trained_u_dim = u_dim_hint\n",
    "\n",
    "    if (u_dim_hint) != trained_u_dim:\n",
    "        print(f\"[Warn] u_dim mismatch (map={u_dim_hint}, ckpt={trained_u_dim}); using ckpt value for eval.\")\n",
    "\n",
    "    model = KoopmanNet(layers, trained_Nkoop, trained_u_dim).to(device)\n",
    "    model.load_state_dict(sd, strict=True)\n",
    "\n",
    "    enc_dim_actual = trained_Nkoop - state_dim\n",
    "    return model, trained_u_dim, enc_dim_actual, trained_Nkoop\n",
    "\n",
    "def gmean(vals, eps: float = 1e-12) -> float:\n",
    "    \"\"\"Geometric mean with small epsilon for numerical stability.\"\"\"\n",
    "    arr = np.asarray(list(vals), dtype=float)\n",
    "    arr = np.maximum(arr, eps)\n",
    "    return float(np.exp(np.mean(np.log(arr))))\n",
    "\n",
    "# =========================\n",
    "# 1) Load CSV log\n",
    "# =========================\n",
    "csv_log_path = os.path.join(\"..\", \"log\", project_name, \"koopman_results_log.csv\")\n",
    "assert os.path.exists(csv_log_path), f\"CSV log not found: {csv_log_path}\"\n",
    "log_df = pd.read_csv(csv_log_path)\n",
    "\n",
    "# types / back-compat\n",
    "for col in [\"use_covariance_loss\", \"use_control_loss\"]:\n",
    "    if col in log_df.columns:\n",
    "        log_df[col] = log_df[col].astype(int)\n",
    "\n",
    "for k, t in {\"env_name\": str, \"encode_dim\": int, \"seed\": int, \"model_path\": str}.items():\n",
    "    if k in log_df.columns:\n",
    "        log_df[k] = log_df[k].astype(t)\n",
    "\n",
    "assert \"train_samples\" in log_df.columns, \\\n",
    "    \"Your log must include a 'train_samples' column for two-factor comparison.\"\n",
    "\n",
    "# =========================\n",
    "# 2) Infer state_dim per env & allowed dims (for FILTERING only)\n",
    "# =========================\n",
    "env_state_dim = {}          # env -> state_dim (if dataset exists)\n",
    "dataset_found = {}          # env -> bool\n",
    "encode_dims_allowed = {}    # env -> list of actual latent sizes (if known)\n",
    "\n",
    "for env in envs:\n",
    "    Ksteps_env = 1 if env == \"Polynomial\" else ksteps\n",
    "    # Try train sizes present in the CSV for this env first\n",
    "    csv_train_sizes = sorted(set(log_df[log_df[\"env_name\"] == env][\"train_samples\"])) \\\n",
    "                      if \"env_name\" in log_df.columns else None\n",
    "    dataset_path = find_dataset_path(env, normalize, Ksteps_env, m, candidate_Ntrains=csv_train_sizes)\n",
    "\n",
    "    if dataset_path is not None:\n",
    "        data_dict = torch.load(dataset_path, weights_only=False)\n",
    "        full_dim  = data_dict[\"Ktest_data\"].shape[2]\n",
    "        u_dim     = u_dim_map.get(env, 0)\n",
    "        state_dim = full_dim - u_dim\n",
    "        env_state_dim[env] = state_dim\n",
    "        dataset_found[env] = True\n",
    "\n",
    "        if encode_dim_mode == \"multiplier\":\n",
    "            encode_dims_allowed[env] = [mult * state_dim for mult in encode_dims]\n",
    "        else:\n",
    "            encode_dims_allowed[env] = list(encode_dims)\n",
    "    else:\n",
    "        env_state_dim[env] = None\n",
    "        dataset_found[env] = False\n",
    "        encode_dims_allowed[env] = list(encode_dims) if encode_dim_mode == \"multiplier\" else list(encode_dims)\n",
    "\n",
    "# =========================\n",
    "# 3) Filter log rows with per-env train sizes & encode_dim_mode\n",
    "# =========================\n",
    "def _row_ok(r):\n",
    "    env = r.get(\"env_name\")\n",
    "    if env not in envs:\n",
    "        return False\n",
    "\n",
    "    # per-env train sizes\n",
    "    env_train_list = set(train_samples_per_env.get(env, default_train_samples))\n",
    "    if r.get(\"train_samples\") not in env_train_list:\n",
    "        return False\n",
    "\n",
    "    # seed\n",
    "    if r.get(\"seed\") not in set(seeds):\n",
    "        return False\n",
    "\n",
    "    # loss toggles (if present)\n",
    "    if \"use_covariance_loss\" in r.index and r.get(\"use_covariance_loss\") not in set(cov_regs):\n",
    "        return False\n",
    "    if \"use_control_loss\" in r.index and r.get(\"use_control_loss\") not in set(ctrl_regs):\n",
    "        return False\n",
    "\n",
    "    # encode_dim_mode compatibility\n",
    "    row_mode = r.get(\"encode_dim_mode\", None)\n",
    "    if not _mode_matches(row_mode, encode_dim_mode):\n",
    "        return False\n",
    "\n",
    "    # encode_dim checks (tolerant)\n",
    "    row_enc = r.get(\"encode_dim\")\n",
    "    if pd.isna(row_enc):\n",
    "        return False\n",
    "    try:\n",
    "        row_enc = int(row_enc)\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "    st_dim = env_state_dim.get(env, None)\n",
    "    if st_dim is not None:\n",
    "        # If we know state_dim, accept either representation:\n",
    "        actual_allowed = [mult * st_dim for mult in encode_dims] \\\n",
    "                         if encode_dim_mode == \"multiplier\" else list(encode_dims)\n",
    "        multiplier_allowed = set(encode_dims)\n",
    "        is_ok = (row_enc in set(actual_allowed)) or (row_enc in multiplier_allowed)\n",
    "    else:\n",
    "        # dataset missing → be permissive but still gated by configured list\n",
    "        if encode_dim_mode == \"multiplier\":\n",
    "            is_ok = (row_enc in set(encode_dims))  # treat logged value as multiplier\n",
    "        else:\n",
    "            is_ok = True  # can't validate absolute without state_dim\n",
    "    return is_ok\n",
    "\n",
    "filtered_df = log_df[log_df.apply(_row_ok, axis=1)]\n",
    "\n",
    "if filtered_df.empty:\n",
    "    print(\"[Error] No models found after filtering.\")\n",
    "    print(\"Available envs in log:\", sorted(log_df[\"env_name\"].unique()))\n",
    "    print(\"Per-env requested train_samples:\", train_samples_per_env)\n",
    "    if \"train_samples\" in log_df.columns:\n",
    "        print(\"Available train_samples in log:\", sorted(log_df[\"train_samples\"].unique()))\n",
    "    print(\"Allowed encode dims per env:\")\n",
    "    for e, dims in encode_dims_allowed.items():\n",
    "        print(f\"  {e}: {dims}\")\n",
    "    if \"encode_dim_mode\" in log_df.columns:\n",
    "        print(\"Available encode_dim_mode:\", sorted(log_df[\"encode_dim_mode\"].astype(str).unique()))\n",
    "    print(\"Available seeds:\", sorted(log_df[\"seed\"].unique()))\n",
    "    if \"use_covariance_loss\" in log_df.columns:\n",
    "        print(\"Available use_covariance_loss:\", sorted(log_df[\"use_covariance_loss\"].unique()))\n",
    "    if \"use_control_loss\" in log_df.columns:\n",
    "        print(\"Available use_control_loss:\", sorted(log_df[\"use_control_loss\"].unique()))\n",
    "    print(\"Datasets found by env:\", {e: dataset_found.get(e, False) for e in envs})\n",
    "    if any(not dataset_found.get(e, False) for e in envs):\n",
    "        print(\"Note: When a dataset is missing, encode-dim filtering is relaxed to avoid false negatives.\")\n",
    "    os.makedirs(project_name, exist_ok=True)\n",
    "    pd.DataFrame(columns=[\n",
    "        \"Environment\",\"TrainSamples\",\"EncodeDim\",\"UseCovLoss\",\"UseControlLoss\",\n",
    "        \"WeightedError\",\"NormalizedCovLoss\"\n",
    "    ]).to_csv(os.path.join(project_name, \"evaluation_summary.csv\"), index=False)\n",
    "    sys.exit(0)\n",
    "\n",
    "# =========================\n",
    "# 4) Index checkpoints by (env, Ntrain, enc_dim, cov, ctrl)\n",
    "# =========================\n",
    "results = {}\n",
    "for _, row in filtered_df.iterrows():\n",
    "    key = (\n",
    "        row[\"env_name\"],\n",
    "        int(row[\"train_samples\"]),\n",
    "        int(row[\"encode_dim\"]),\n",
    "        row.get(\"use_covariance_loss\", 0),\n",
    "        row.get(\"use_control_loss\", 0),\n",
    "    )\n",
    "    results.setdefault(key, []).append((int(row[\"seed\"]), row[\"model_path\"]))\n",
    "\n",
    "# to optionally record actual z per key (for labels)\n",
    "results[\"_actual_z_map\"] = {}\n",
    "\n",
    "# =========================\n",
    "# 5) Evaluate & aggregate (geom. mean across seeds)\n",
    "# =========================\n",
    "agg_metrics = {}\n",
    "\n",
    "for env in envs:\n",
    "    Ksteps_env = 1 if env == \"Polynomial\" else ksteps\n",
    "    if env == 'G1' or env == 'Go2':\n",
    "        gamma = 0.99\n",
    "    # Prefer train sizes present in CSV for this env when locating dataset\n",
    "    csv_train_sizes = sorted(set(filtered_df[filtered_df[\"env_name\"] == env][\"train_samples\"])) \\\n",
    "                      if \"env_name\" in filtered_df.columns else None\n",
    "    dataset_path = find_dataset_path(env, normalize, Ksteps_env, m, candidate_Ntrains=csv_train_sizes)\n",
    "    if dataset_path is None:\n",
    "        print(f\"[Warning] Missing dataset for {env}, skipping.\")\n",
    "        continue\n",
    "\n",
    "    data_dict = torch.load(dataset_path, weights_only=False)\n",
    "    test_data = torch.from_numpy(data_dict[\"Ktest_data\"]).float().to(device)\n",
    "\n",
    "    u_dim_hint = u_dim_map.get(env, None)\n",
    "    state_dim  = test_data.shape[2] - (u_dim_hint or 0)\n",
    "\n",
    "    for key, checkpoints in list(results.items()):\n",
    "        if key == \"_actual_z_map\":\n",
    "            continue\n",
    "        if key[0] != env:\n",
    "            continue\n",
    "        _, Ntrain, logged_enc_dim, cov_reg, ctrl_reg = key\n",
    "\n",
    "        w_errs, cov_losses = [], []\n",
    "        actual_z_recorded = False\n",
    "\n",
    "        for seed_val, path in checkpoints:\n",
    "            if not os.path.exists(path):\n",
    "                print(f\"[Warning] Missing checkpoint: {path}\")\n",
    "                continue\n",
    "\n",
    "            chkpt = torch.load(path, map_location=device, weights_only=False)\n",
    "            try:\n",
    "                model, u_dim_eval, enc_dim_actual, trained_Nkoop = build_model_from_checkpoint(\n",
    "                    chkpt, state_dim, u_dim_hint, device\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(f\"[Error] Failed to build/load model from {path}: {e}\")\n",
    "                continue\n",
    "\n",
    "            # Record actual z once for this key (for labels/tables)\n",
    "            if not actual_z_recorded:\n",
    "                results[\"_actual_z_map\"][key] = enc_dim_actual\n",
    "                actual_z_recorded = True\n",
    "\n",
    "            w_err, c_loss = evaluate_model(model, test_data, u_dim_eval, gamma, state_dim, device)\n",
    "            w_errs.append(w_err)\n",
    "            cov_losses.append(c_loss)\n",
    "\n",
    "        if not w_errs:\n",
    "            continue\n",
    "\n",
    "        # Geom. mean across seeds\n",
    "        agg_metrics[key] = {\n",
    "            \"WeightedError_mean\":     gmean(w_errs),\n",
    "            \"NormalizedCovLoss_mean\": gmean(np.array(cov_losses) + 1e-12),\n",
    "        }\n",
    "\n",
    "# =========================\n",
    "# 6) Save summary CSV\n",
    "# =========================\n",
    "rows = []\n",
    "for (env, Ntrain, logged_enc_dim, cov_reg, ctrl_reg), metrics in agg_metrics.items():\n",
    "    z_actual = results[\"_actual_z_map\"].get((env, Ntrain, logged_enc_dim, cov_reg, ctrl_reg), logged_enc_dim)\n",
    "    rows.append({\n",
    "        \"Environment\":       env,\n",
    "        \"TrainSamples\":      Ntrain,\n",
    "        \"EncodeDim\":         z_actual,        # report actual z used for the loaded model\n",
    "        \"UseCovLoss\":        cov_reg,\n",
    "        \"UseControlLoss\":    ctrl_reg,\n",
    "        \"WeightedError\":     metrics[\"WeightedError_mean\"],\n",
    "        \"NormalizedCovLoss\": metrics[\"NormalizedCovLoss_mean\"]\n",
    "    })\n",
    "\n",
    "df_summary = pd.DataFrame(rows)\n",
    "os.makedirs(project_name, exist_ok=True)\n",
    "summary_path = os.path.join(project_name, \"evaluation_summary.csv\")\n",
    "df_summary.to_csv(summary_path, index=False)\n",
    "print(f\"Saved summary to {summary_path}\")\n",
    "if not df_summary.empty:\n",
    "    display(df_summary)\n",
    "\n",
    "# ======================================================\n",
    "# 7) NEW PLOTS (loss filter depends on whether env has control)\n",
    "#    A) TrainSamples vs averaged error (geom. mean across EncodeDim)\n",
    "#    B) EncodeDim    vs averaged error (geom. mean across TrainSamples)\n",
    "# ======================================================\n",
    "if df_summary.empty:\n",
    "    print(\"[Skip] No rows in df_summary; nothing to plot.\")\n",
    "else:\n",
    "    for env in envs:\n",
    "        # ---- choose rows: cov=1 AND (ctrl=1 if env has control; otherwise ignore ctrl) ----\n",
    "        if env_has_control(env) and (\"UseControlLoss\" in df_summary.columns):\n",
    "            sub = df_summary[\n",
    "                (df_summary[\"Environment\"] == env) &\n",
    "                (df_summary[\"UseCovLoss\"] == 1) &\n",
    "                (df_summary[\"UseControlLoss\"] == 1)\n",
    "            ].copy()\n",
    "            title_suffix = \"(cov=1, ctrl=1)\"\n",
    "        else:\n",
    "            # e.g., Polynomial: no control loss — only require covariance loss on\n",
    "            sub = df_summary[\n",
    "                (df_summary[\"Environment\"] == env) &\n",
    "                (df_summary[\"UseCovLoss\"] == 1)\n",
    "            ].copy()\n",
    "            title_suffix = \"(cov=1; no control)\"\n",
    "\n",
    "        if sub.empty:\n",
    "            print(f\"[Skip] No eligible rows for env {env} with filter {title_suffix}.\")\n",
    "            continue\n",
    "\n",
    "        out_dir = os.path.join(project_name, env)\n",
    "        os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "        # -------- Plot A: TrainSamples vs averaged error --------\n",
    "        grp_TS = (\n",
    "            sub.groupby(\"TrainSamples\", as_index=False)\n",
    "               .agg(WeightedError_gmean=(\"WeightedError\", lambda s: gmean(s.values)))\n",
    "               .sort_values(\"TrainSamples\")\n",
    "        )\n",
    "        if not grp_TS.empty:\n",
    "            fig, ax = plt.subplots(figsize=(8, 6))\n",
    "            ax.plot(grp_TS[\"TrainSamples\"], grp_TS[\"WeightedError_gmean\"], marker=\"o\")\n",
    "            ax.set_xscale(\"linear\")\n",
    "            ax.set_yscale(\"log\")\n",
    "            ax.set_xlabel(\"Train Samples (Ktrain)\")\n",
    "            ax.set_ylabel(\"Weighted Prediction Error (MSE, geom. mean)\")\n",
    "            ax.set_title(f\"{env} — TrainSamples vs Error {title_suffix}\")\n",
    "            ax.grid(True, which=\"both\", ls=\"--\", alpha=0.6)\n",
    "            fig.tight_layout()\n",
    "            out_path = os.path.join(out_dir, f\"{env}_TrainSamples_vs_Error.png\")\n",
    "            plt.savefig(out_path, dpi=300)\n",
    "            plt.close(fig)\n",
    "            print(f\"Saved: {out_path}\")\n",
    "        else:\n",
    "            print(f\"[Skip] No data for TrainSamples plot in {env}.\")\n",
    "\n",
    "        # -------- Plot B: EncodeDim vs averaged error --------\n",
    "        grp_Z = (\n",
    "            sub.groupby(\"EncodeDim\", as_index=False)\n",
    "               .agg(WeightedError_gmean=(\"WeightedError\", lambda s: gmean(s.values)))\n",
    "               .sort_values(\"EncodeDim\")\n",
    "        )\n",
    "        if not grp_Z.empty:\n",
    "            fig, ax = plt.subplots(figsize=(8, 6))\n",
    "            ax.plot(grp_Z[\"EncodeDim\"], grp_Z[\"WeightedError_gmean\"], marker=\"s\")\n",
    "            ax.set_xscale(\"linear\")\n",
    "            ax.set_yscale(\"log\")\n",
    "            ax.set_xlabel(\"Encode Dimension (z)\")\n",
    "            ax.set_ylabel(\"Weighted Prediction Error (MSE, geom. mean)\")\n",
    "            ax.set_title(f\"{env} — EncodeDim vs Error {title_suffix}\")\n",
    "            ax.grid(True, which=\"both\", ls=\"--\", alpha=0.6)\n",
    "            fig.tight_layout()\n",
    "            out_path = os.path.join(out_dir, f\"{env}_EncodeDim_vs_Error.png\")\n",
    "            plt.savefig(out_path, dpi=300)\n",
    "            plt.close(fig)\n",
    "            print(f\"Saved: {out_path}\")\n",
    "        else:\n",
    "            print(f\"[Skip] No data for EncodeDim plot in {env}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9c5ead",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "koopman",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

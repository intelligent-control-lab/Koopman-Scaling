{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3deb2ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Skip] No dataset found for Franka.\n",
      "[Skip] No dataset found for DoublePendulum.\n",
      "[Skip] No dataset found for Polynomial.\n",
      "[Skip] No dataset found for Kinova.\n",
      "[Skip] No dataset found for G1.\n",
      "[Skip] No dataset found for Go2.\n",
      "Saved summary to Sep_21/evaluation_summary.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Environment</th>\n",
       "      <th>TrainSamples</th>\n",
       "      <th>EncodeDim</th>\n",
       "      <th>UseCovLoss</th>\n",
       "      <th>UseControlLoss</th>\n",
       "      <th>WeightedError</th>\n",
       "      <th>NormalizedCovLoss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DampingPendulum</td>\n",
       "      <td>1000</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.013001</td>\n",
       "      <td>4868574.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Environment  TrainSamples  EncodeDim  UseCovLoss  UseControlLoss  \\\n",
       "0  DampingPendulum          1000          2           0               0   \n",
       "\n",
       "   WeightedError  NormalizedCovLoss  \n",
       "0       0.013001          4868574.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Skip] No eligible rows for DampingPendulum (cov=1, ctrl=1)\n",
      "[Skip] No eligible rows for Franka (cov=1, ctrl=1)\n",
      "[Skip] No eligible rows for DoublePendulum (cov=1, ctrl=1)\n",
      "[Skip] No eligible rows for Polynomial (cov=1)\n",
      "[Skip] No eligible rows for Kinova (cov=1, ctrl=1)\n",
      "[Skip] No eligible rows for G1 (cov=1, ctrl=1)\n",
      "[Skip] No eligible rows for Go2 (cov=1, ctrl=1)\n",
      "[Skip] Scaling-law: no rows for DampingPendulum\n",
      "[Skip] Scaling-law: no rows for Franka\n",
      "[Skip] Scaling-law: no rows for DoublePendulum\n",
      "[Skip] Scaling-law: no rows for Polynomial\n",
      "[Skip] Scaling-law: no rows for Kinova\n",
      "[Skip] Scaling-law: no rows for G1\n",
      "[Skip] Scaling-law: no rows for Go2\n",
      "[Skip] No scaling-law fits produced.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "from scipy.optimize import curve_fit\n",
    "\n",
    "# ---- Imports ----\n",
    "sys.path.append(\"../scripts\")\n",
    "sys.path.append(\"../utility\")\n",
    "from network import KoopmanNet\n",
    "\n",
    "# ============================\n",
    "# Minimal configuration\n",
    "# ============================\n",
    "PROJECT_NAME = \"Sep_21\"\n",
    "DEVICE       = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "ENVS         = [\"DampingPendulum\", \"Franka\", \"DoublePendulum\", \"Polynomial\", \"Kinova\", \"G1\", \"Go2\"]\n",
    "SEEDS        = [17382, 76849, 20965, 84902, 51194]\n",
    "\n",
    "TRAIN_SAMPLES = [1000, 4000, 16000, 64000, 140000]\n",
    "\n",
    "M_POLY = 100\n",
    "\n",
    "U_DIM = {\"Franka\": 7, \"DoublePendulum\": 2, \"DampingPendulum\": 1, \"G1\": 23, \"Go2\": 12, \"Kinova\": 7}\n",
    "\n",
    "NORMALIZE = {\"G1\": \"norm\", \"Go2\": \"norm\"}\n",
    "\n",
    "GAMMA_DEFAULT   = 0.8\n",
    "GAMMA_OVERRIDES = {\"G1\": 0.99, \"Go2\": 0.99}\n",
    "\n",
    "REL_MULT_TARGETS = [1, 2, 4, 8, 16]\n",
    "\n",
    "# ============================\n",
    "# Helpers\n",
    "# ============================\n",
    "def gmean(vals, eps: float = 1e-12) -> float:\n",
    "    arr = np.asarray(list(vals), dtype=float)\n",
    "    arr = np.maximum(arr, eps)\n",
    "    return float(np.exp(np.mean(np.log(arr))))\n",
    "\n",
    "def env_has_control(env: str) -> bool:\n",
    "    return U_DIM.get(env, 0) > 0\n",
    "\n",
    "def find_dataset_path(env: str, ksteps: int, m_val: int) -> str | None:\n",
    "    \"\"\"Return the FIRST dataset path found for this env over its adjusted train sizes.\"\"\"\n",
    "    norm = NORMALIZE.get(env, \"nonorm\")\n",
    "    if env == \"Polynomial\":\n",
    "        path = os.path.join(\n",
    "            \"..\", \"data\", \"datasets\",\n",
    "            f\"dataset_{env}_{norm}_m_{m_val}_Ktrain_140000_Kval_20000_Ktest_20000_Ksteps_{ksteps}.pt\",\n",
    "        )\n",
    "    else:\n",
    "        path = os.path.join(\n",
    "            \"..\", \"data\", \"datasets\",\n",
    "            f\"dataset_{env}_{norm}_Ktrain_140000_Kval_20000_Ktest_20000_Ksteps_{ksteps}.pt\",\n",
    "        )\n",
    "    if os.path.exists(path):\n",
    "        return path\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def evaluate_model(model, data, u_dim: int | None, gamma: float, state_dim: int, device) -> tuple[float, float]:\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        T = data.shape[0]\n",
    "        if u_dim is None or u_dim == 0:\n",
    "            X = model.encode(data[0].to(device))\n",
    "        else:\n",
    "            X = model.encode(data[0, :, u_dim:].to(device))  # encode state-only slice\n",
    "        z0 = X[:, state_dim:]\n",
    "\n",
    "        # Geometrically weighted rollout MSE on states\n",
    "        beta, beta_sum = 1.0, 0.0\n",
    "        wloss = 0.0\n",
    "        for t in range(T - 1):\n",
    "            if u_dim is None or u_dim == 0:\n",
    "                X      = model.forward(X, None)\n",
    "                target = data[t + 1].to(device)\n",
    "            else:\n",
    "                X      = model.forward(X, data[t, :, :u_dim].to(device))\n",
    "                target = data[t + 1, :, u_dim:].to(device)\n",
    "            err   = nn.MSELoss()(X[:, :state_dim], target)\n",
    "            wloss = wloss + beta * err\n",
    "            beta_sum += beta\n",
    "            beta *= gamma\n",
    "        wloss = wloss / max(beta_sum, 1e-12)\n",
    "\n",
    "        # Off-diagonal covariance loss on initial encoding (normalized)\n",
    "        zc = z0 - z0.mean(dim=0, keepdim=True)\n",
    "        C  = (zc.t() @ zc) / max(zc.size(0) - 1, 1)\n",
    "        off = C - torch.diag(torch.diag(C))\n",
    "        cov = torch.norm(off, p='fro') ** 2\n",
    "        zdim = max(X.shape[1] - state_dim, 1)\n",
    "        cov_norm = cov.item() / (zdim * (zdim - 1)) if zdim > 1 else cov.item()\n",
    "        return float(wloss.item()), float(cov_norm)\n",
    "\n",
    "\n",
    "def build_model_from_checkpoint(chkpt: dict, state_dim: int, u_dim_hint: int | None, device):\n",
    "    layers = chkpt[\"layer\"]\n",
    "    sd     = chkpt[\"model\"]\n",
    "\n",
    "    # Nkoop from lA\n",
    "    if \"lA.weight\" not in sd or sd[\"lA.weight\"].ndim != 2:\n",
    "        raise RuntimeError(\"Checkpoint missing a square lA.weight for Nkoop inference\")\n",
    "    Nkoop = int(sd[\"lA.weight\"].shape[0])\n",
    "\n",
    "    # u_dim from lB if present\n",
    "    u_dim_trained = int(sd[\"lB.weight\"].shape[1]) if (\"lB.weight\" in sd and sd[\"lB.weight\"].ndim == 2) else None#int(u_dim_hint or 0)\n",
    "\n",
    "    model = KoopmanNet(layers, Nkoop, u_dim_trained).to(device)\n",
    "    model.load_state_dict(sd, strict=True)\n",
    "    enc_dim = Nkoop - state_dim\n",
    "    return model, u_dim_trained, enc_dim, Nkoop\n",
    "\n",
    "\n",
    "# ============================\n",
    "# 1) Load CSV log\n",
    "# ============================\n",
    "log_csv = os.path.join(\"..\", \"log\", PROJECT_NAME, \"koopman_results_log.csv\")\n",
    "assert os.path.exists(log_csv), f\"CSV log not found: {log_csv}\"\n",
    "log = pd.read_csv(log_csv)\n",
    "\n",
    "# Types & light cleanup\n",
    "for col in [\"use_covariance_loss\", \"use_control_loss\"]:\n",
    "    if col in log.columns:\n",
    "        log[col] = log[col].astype(int)\n",
    "for k, t in {\"env_name\": str, \"seed\": int, \"model_path\": str}.items():\n",
    "    if k in log.columns:\n",
    "        log[k] = log[k].astype(t)\n",
    "assert \"train_samples\" in log.columns, \"Log must include 'train_samples'\"\n",
    "\n",
    "# ============================\n",
    "# 2) Infer state_dim per env from any available dataset\n",
    "# ============================\n",
    "env_state_dim: dict[str, int | None] = {}\n",
    "for env in ENVS:\n",
    "    ksteps = 1 if env == \"Polynomial\" else 15\n",
    "    ds_path = find_dataset_path(env, ksteps, M_POLY)\n",
    "    if ds_path is None:\n",
    "        env_state_dim[env] = None\n",
    "        continue\n",
    "    data = torch.load(ds_path, weights_only=False)\n",
    "    full_dim = int(data[\"Ktest_data\"].shape[2])\n",
    "    u = U_DIM.get(env, 0)\n",
    "    env_state_dim[env] = full_dim - u\n",
    "\n",
    "# ============================\n",
    "# 3) Filter rows (lean rules)\n",
    "# ============================\n",
    "def row_ok(r) -> bool:\n",
    "    env = r.get(\"env_name\")\n",
    "    if env not in ENVS:\n",
    "        return False\n",
    "    if r.get(\"seed\") not in SEEDS:\n",
    "        return False\n",
    "    if r.get(\"train_samples\") not in set(TRAIN_SAMPLES):\n",
    "        return False\n",
    "    if env == \"Polynomial\":\n",
    "        try:\n",
    "            if int(r.get(\"m\")) != M_POLY:\n",
    "                return False\n",
    "        except Exception:\n",
    "            return False\n",
    "    if \"use_covariance_loss\" in r.index and r.get(\"use_covariance_loss\") not in {0, 1}:\n",
    "        return False\n",
    "    if \"use_control_loss\" in r.index and r.get(\"use_control_loss\") not in {0, 1}:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "filtered = log[log.apply(row_ok, axis=1)].copy()\n",
    "if filtered.empty:\n",
    "    print(\"[Error] No models after filtering. Check envs/train_samples/seeds.\")\n",
    "    out_dir = PROJECT_NAME\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    pd.DataFrame(columns=[\n",
    "        \"Environment\",\"TrainSamples\",\"EncodeDim\",\"UseCovLoss\",\"UseControlLoss\",\n",
    "        \"WeightedError\",\"NormalizedCovLoss\"\n",
    "    ]).to_csv(os.path.join(out_dir, \"evaluation_summary.csv\"), index=False)\n",
    "    raise SystemExit(0)\n",
    "\n",
    "# ============================\n",
    "# 4) Index checkpoints by (env, Ntrain, enc_dim_logged, cov, ctrl)\n",
    "# ============================\n",
    "index = {}\n",
    "for _, r in filtered.iterrows():\n",
    "    key = (\n",
    "        r[\"env_name\"], int(r[\"train_samples\"]), int(r.get(\"encode_dim\", -1)),\n",
    "        int(r.get(\"use_covariance_loss\", 0)), int(r.get(\"use_control_loss\", 0)),\n",
    "    )\n",
    "    index.setdefault(key, []).append((int(r[\"seed\"]), r[\"model_path\"]))\n",
    "index[\"_z_actual\"] = {}\n",
    "\n",
    "# ============================\n",
    "# 5) Evaluate & aggregate (geometric mean across seeds)\n",
    "# ============================\n",
    "agg = {}\n",
    "for env in ENVS:\n",
    "    ksteps = 1 if env == \"Polynomial\" else 15\n",
    "    gamma  = GAMMA_OVERRIDES.get(env, GAMMA_DEFAULT)\n",
    "\n",
    "    ds_path = find_dataset_path(env, ksteps, M_POLY)\n",
    "    if ds_path is None:\n",
    "        print(f\"[Skip] No dataset found for {env}.\")\n",
    "        continue\n",
    "\n",
    "    d = torch.load(ds_path, weights_only=False)\n",
    "    test = torch.from_numpy(d[\"Ktest_data\"]).float().to(DEVICE)\n",
    "    u_hint = U_DIM.get(env, 0)\n",
    "    state_dim = int(test.shape[2]) - int(u_hint)\n",
    "\n",
    "    for key, ckpts in list(index.items()):\n",
    "        if key == \"_z_actual\" or key[0] != env:\n",
    "            continue\n",
    "        _, N, enc_logged, cov_reg, ctrl_reg = key\n",
    "\n",
    "        w_errs, covs = [], []\n",
    "        z_recorded = False\n",
    "        for seed, path in ckpts:\n",
    "            if not isinstance(path, str) or not os.path.exists(path):\n",
    "                print(f\"[Warn] Missing checkpoint: {path}\")\n",
    "                continue\n",
    "            try:\n",
    "                chkpt = torch.load(path, map_location=DEVICE, weights_only=False)\n",
    "                model, u_eval, z_actual, _ = build_model_from_checkpoint(chkpt, state_dim, u_hint, DEVICE)\n",
    "                if not z_recorded:\n",
    "                    index[\"_z_actual\"][key] = z_actual\n",
    "                    z_recorded = True\n",
    "                w, c = evaluate_model(model, test, u_eval, gamma, state_dim, DEVICE)\n",
    "                w_errs.append(w); covs.append(c)\n",
    "            except Exception as e:\n",
    "                print(f\"[Error] Load/eval failed for {path}: {e}\")\n",
    "                continue\n",
    "        if not w_errs:\n",
    "            continue\n",
    "        agg[key] = {\n",
    "            \"WeightedError_mean\":     gmean(w_errs),\n",
    "            \"NormalizedCovLoss_mean\": gmean(np.array(covs) + 1e-12),\n",
    "        }\n",
    "\n",
    "# ============================\n",
    "# 6) Save summary CSV\n",
    "# ============================\n",
    "rows = []\n",
    "for (env, N, enc_logged, cov_reg, ctrl_reg), met in agg.items():\n",
    "    z = index[\"_z_actual\"].get((env, N, enc_logged, cov_reg, ctrl_reg), enc_logged)\n",
    "    rows.append({\n",
    "        \"Environment\": env,\n",
    "        \"TrainSamples\": N,\n",
    "        \"EncodeDim\": z,\n",
    "        \"UseCovLoss\": cov_reg,\n",
    "        \"UseControlLoss\": ctrl_reg,\n",
    "        \"WeightedError\": met[\"WeightedError_mean\"],\n",
    "        \"NormalizedCovLoss\": met[\"NormalizedCovLoss_mean\"],\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "os.makedirs(PROJECT_NAME, exist_ok=True)\n",
    "out_csv = os.path.join(PROJECT_NAME, \"evaluation_summary.csv\")\n",
    "df.to_csv(out_csv, index=False)\n",
    "print(f\"Saved summary to {out_csv}\")\n",
    "if not df.empty:\n",
    "    display(df)\n",
    "\n",
    "# ============================\n",
    "# 7) Per-env plots (cov=1 and ctrl=1 if env has control)\n",
    "# ============================\n",
    "if not df.empty:\n",
    "    for env in ENVS:\n",
    "        if env_has_control(env) and (\"UseControlLoss\" in df.columns):\n",
    "            sub = df[(df.Environment == env) & (df.UseCovLoss == 1) & (df.UseControlLoss == 1)].copy()\n",
    "            tag = \"(cov=1, ctrl=1)\"\n",
    "        else:\n",
    "            sub = df[(df.Environment == env) & (df.UseCovLoss == 1)].copy()\n",
    "            tag = \"(cov=1)\"\n",
    "        if sub.empty:\n",
    "            print(f\"[Skip] No eligible rows for {env} {tag}\")\n",
    "            continue\n",
    "\n",
    "        out_dir = os.path.join(PROJECT_NAME, env)\n",
    "        os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "        # A) TrainSamples vs error\n",
    "        gA = (\n",
    "            sub.groupby(\"TrainSamples\", as_index=False)\n",
    "               .agg(WeightedError_gmean=(\"WeightedError\", lambda s: gmean(s.values)))\n",
    "               .sort_values(\"TrainSamples\")\n",
    "        )\n",
    "        if not gA.empty:\n",
    "            fig, ax = plt.subplots(figsize=(8, 6))\n",
    "            ax.plot(gA[\"TrainSamples\"], gA[\"WeightedError_gmean\"], marker=\"o\")\n",
    "            ax.set_yscale(\"log\")\n",
    "            ax.set_xlabel(\"Train Samples (Ktrain)\")\n",
    "            ax.set_ylabel(\"Weighted Prediction Error (MSE, geom. mean)\")\n",
    "            ax.set_title(f\"{env} — TrainSamples vs Error {tag}\")\n",
    "            ax.grid(True, which=\"both\", ls=\"--\", alpha=0.6)\n",
    "            fig.tight_layout(); p = os.path.join(out_dir, f\"{env}_TrainSamples_vs_Error.png\")\n",
    "            plt.savefig(p, dpi=300); plt.close(fig); print(f\"Saved: {p}\")\n",
    "\n",
    "        # B) EncodeDim vs error\n",
    "        gB = (\n",
    "            sub.groupby(\"EncodeDim\", as_index=False)\n",
    "               .agg(WeightedError_gmean=(\"WeightedError\", lambda s: gmean(s.values)))\n",
    "               .sort_values(\"EncodeDim\")\n",
    "        )\n",
    "        if not gB.empty:\n",
    "            fig, ax = plt.subplots(figsize=(8, 6))\n",
    "            ax.plot(gB[\"EncodeDim\"], gB[\"WeightedError_gmean\"], marker=\"s\")\n",
    "            ax.set_xscale(\"linear\"); ax.set_yscale(\"log\")\n",
    "            ax.set_xlabel(\"Encode Dimension (z)\")\n",
    "            ax.set_ylabel(\"Weighted Prediction Error (MSE, geom. mean)\")\n",
    "            ax.set_title(f\"{env} — EncodeDim vs Error {tag}\")\n",
    "            ax.grid(True, which=\"both\", ls=\"--\", alpha=0.6)\n",
    "            fig.tight_layout(); p = os.path.join(out_dir, f\"{env}_EncodeDim_vs_Error.png\")\n",
    "            plt.savefig(p, dpi=300); plt.close(fig); print(f\"Saved: {p}\")\n",
    "\n",
    "# ============================\n",
    "# 8) Combined plot (normalized error vs relative multiplier)\n",
    "# ============================\n",
    "if not df.empty:\n",
    "    def nearest_rel_mult(env: str, z_abs: float) -> float:\n",
    "        st = env_state_dim.get(env, None)\n",
    "        if not st or st <= 0:\n",
    "            return float(\"nan\")\n",
    "        r = float(z_abs) / float(st)\n",
    "        return float(min(REL_MULT_TARGETS, key=lambda m: abs(m - r)))\n",
    "\n",
    "    rows = []\n",
    "    for env in ENVS:\n",
    "        if env_has_control(env) and (\"UseControlLoss\" in df.columns):\n",
    "            sub = df[(df.Environment == env) & (df.UseCovLoss == 1) & (df.UseControlLoss == 1)].copy()\n",
    "        else:\n",
    "            sub = df[(df.Environment == env) & (df.UseCovLoss == 1)].copy()\n",
    "        if sub.empty:\n",
    "            continue\n",
    "\n",
    "        sub[\"RelMult\"] = sub[\"EncodeDim\"].apply(lambda z: nearest_rel_mult(env, z))\n",
    "        sub = sub.replace([np.inf, -np.inf], np.nan).dropna(subset=[\"RelMult\"])\n",
    "        g = (\n",
    "            sub.groupby([\"Environment\", \"RelMult\"], as_index=False)\n",
    "               .agg(WeightedError_gmean=(\"WeightedError\", lambda s: gmean(s.values)))\n",
    "               .sort_values(\"RelMult\")\n",
    "        )\n",
    "        if g.empty:\n",
    "            continue\n",
    "\n",
    "        # Normalize by error at smallest multiplier\n",
    "        E0 = g.loc[g[\"RelMult\"].idxmin(), \"WeightedError_gmean\"]\n",
    "        g[\"RelError\"] = g[\"WeightedError_gmean\"] / max(E0, 1e-12)\n",
    "\n",
    "        # Log–log slope\n",
    "        if g.shape[0] >= 2:\n",
    "            x = np.log(g[\"RelMult\"].to_numpy(dtype=float))\n",
    "            y = np.log(np.maximum(g[\"RelError\"].to_numpy(dtype=float), 1e-12))\n",
    "            b1, _ = np.polyfit(x, y, 1)\n",
    "            g[\"Slope\"] = float(b1)\n",
    "        else:\n",
    "            g[\"Slope\"] = float(\"nan\")\n",
    "\n",
    "        # Noise proxy: average of top-2 rel errors\n",
    "        k = min(2, len(g))\n",
    "        g[\"NoiseRel\"] = float(g.tail(k)[\"RelError\"].mean())\n",
    "        rows.append(g)\n",
    "\n",
    "    if rows:\n",
    "        GG = pd.concat(rows, ignore_index=True)\n",
    "        fig, ax = plt.subplots(figsize=(8, 6))\n",
    "        for env in sorted(GG[\"Environment\"].unique()):\n",
    "            ge = GG[GG.Environment == env].sort_values(\"RelMult\")\n",
    "            (line,) = ax.plot(ge[\"RelMult\"], ge[\"RelError\"], marker=\"o\", label=f\"{env} (slope={ge['Slope'].iloc[0]:.2f})\")\n",
    "            ax.hlines(ge[\"NoiseRel\"].iloc[0], ge[\"RelMult\"].min(), ge[\"RelMult\"].max(), linestyles=\"dashed\", alpha=0.4, colors=[line.get_color()])\n",
    "        try:\n",
    "            ax.set_xscale(\"log\", base=2)\n",
    "        except TypeError:\n",
    "            ax.set_xscale(\"log\", basex=2)\n",
    "        ax.set_xlabel(\"Relative encode multiplier (z / state_dim, log₂)\")\n",
    "        ax.set_ylabel(\"Relative prediction error (E / E@min multiplier)\")\n",
    "        ax.set_title(\"Normalized error vs relative encode multiplier (strict)\")\n",
    "        ax.grid(True, which=\"both\", ls=\"--\", alpha=0.6)\n",
    "        ax.legend(ncol=2, fontsize=9)\n",
    "        fig.tight_layout()\n",
    "        p = os.path.join(PROJECT_NAME, \"AllEnvs_RelError_vs_RelMultiplier_STRICT.png\")\n",
    "        plt.savefig(p, dpi=300); plt.close(fig); print(f\"Saved combined plot: {p}\")\n",
    "\n",
    "# ============================\n",
    "# 9) Scaling-law fits per env:  E(D) = A * D^{-alpha} + C\n",
    "# ============================\n",
    "if not df.empty:\n",
    "    def strict_subset(dfx: pd.DataFrame, env: str) -> pd.DataFrame:\n",
    "        if env_has_control(env) and (\"UseControlLoss\" in dfx.columns):\n",
    "            return dfx[(dfx.Environment == env) & (dfx.UseCovLoss == 1) & (dfx.UseControlLoss == 1)].copy()\n",
    "        return dfx[(dfx.Environment == env) & (dfx.UseCovLoss == 1)].copy()\n",
    "\n",
    "    def agg_over_z(dfe: pd.DataFrame) -> pd.DataFrame:\n",
    "        return (\n",
    "            dfe.groupby(\"EncodeDim\", as_index=False)\n",
    "               .agg(WeightedError_gmean=(\"WeightedError\", lambda s: gmean(s.values)))\n",
    "               .sort_values(\"EncodeDim\")\n",
    "        )\n",
    "\n",
    "    def scaling_model(D, A, alpha, C):\n",
    "        return A * np.power(D, -alpha) + C\n",
    "\n",
    "    fits = []\n",
    "    for env in ENVS:\n",
    "        sub = strict_subset(df, env)\n",
    "        if sub.empty:\n",
    "            print(f\"[Skip] Scaling-law: no rows for {env}\")\n",
    "            continue\n",
    "        G = agg_over_z(sub)\n",
    "        if len(G) < 3:\n",
    "            print(f\"[Skip] Scaling-law: need >=3 points for {env}, have {len(G)}\")\n",
    "            continue\n",
    "\n",
    "        D = G[\"EncodeDim\"].astype(float).to_numpy()\n",
    "        E = G[\"WeightedError_gmean\"].astype(float).to_numpy()\n",
    "\n",
    "        C0 = float(G.tail(min(2, len(G)))[\"WeightedError_gmean\"].mean())\n",
    "        A0 = max(float(E.max() - C0), 1e-6)\n",
    "        alpha0 = 0.7\n",
    "\n",
    "        try:\n",
    "            popt, pcov = curve_fit(\n",
    "                scaling_model, D, E,\n",
    "                p0=[A0, alpha0, C0],\n",
    "                bounds=([0.0, 0.0, 0.0], [np.inf, 4.0, np.inf]),\n",
    "                maxfev=20000,\n",
    "            )\n",
    "            A_hat, alpha_hat, C_hat = map(float, popt)\n",
    "            perr = np.sqrt(np.maximum(np.diag(pcov), 0.0))\n",
    "            dA, dalpha, dC = map(float, perr)\n",
    "        except Exception as ex:\n",
    "            print(f\"[Warn] curve_fit failed for {env}: {ex}\")\n",
    "            x = np.log(D); y = np.log(np.maximum(E, 1e-12))\n",
    "            b1, b0 = np.polyfit(x, y, 1)\n",
    "            A_hat, alpha_hat, C_hat = float(np.exp(b0)), -float(b1), 0.0\n",
    "            dA = dalpha = dC = float(\"nan\")\n",
    "\n",
    "        E_pred = scaling_model(D, A_hat, alpha_hat, C_hat)\n",
    "        ss_res = float(np.sum((E - E_pred) ** 2))\n",
    "        ss_tot = float(np.sum((E - np.mean(E)) ** 2))\n",
    "        R2_lin = 1.0 - ss_res / ss_tot if ss_tot > 0 else float(\"nan\")\n",
    "\n",
    "        mask = (E > C_hat + 1e-12) & (E_pred > C_hat + 1e-12)\n",
    "        if np.count_nonzero(mask) >= 2:\n",
    "            y_log = np.log(E[mask] - C_hat)\n",
    "            yhat  = np.log(E_pred[mask] - C_hat)\n",
    "            ss_res_l = float(np.sum((y_log - yhat) ** 2))\n",
    "            ss_tot_l = float(np.sum((y_log - np.mean(y_log)) ** 2))\n",
    "            R2_log = 1.0 - ss_res_l / ss_tot_l if ss_tot_l > 0 else float(\"nan\")\n",
    "        else:\n",
    "            R2_log = float(\"nan\")\n",
    "\n",
    "        fits.append({\n",
    "            \"Environment\": env,\n",
    "            \"A\": A_hat, \"A_se\": dA,\n",
    "            \"alpha\": alpha_hat, \"alpha_se\": dalpha,\n",
    "            \"C\": C_hat, \"C_se\": dC,\n",
    "            \"R2_linear\": R2_lin, \"R2_log_after_C\": R2_log,\n",
    "            \"n_points\": int(len(D)),\n",
    "            \"D_min\": float(np.min(D)), \"D_max\": float(np.max(D)),\n",
    "        })\n",
    "\n",
    "        # Plot overlay\n",
    "        out_dir = os.path.join(PROJECT_NAME, env)\n",
    "        os.makedirs(out_dir, exist_ok=True)\n",
    "        Dg = np.geomspace(max(1e-6, D.min()), D.max(), 256)\n",
    "        Eg = scaling_model(Dg, A_hat, alpha_hat, C_hat)\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(7, 5))\n",
    "        ax.plot(D, E, \"o\", label=\"geom. mean (per z)\")\n",
    "        ax.plot(Dg, Eg, \"-\", label=f\"fit: A={A_hat:.3g}, α={alpha_hat:.2f}, C={C_hat:.3g}\\nR²={R2_lin:.2f} (lin), {R2_log:.2f} (log−C)\")\n",
    "        ax.hlines(C_hat, D.min(), D.max(), linestyles=\"dashed\", alpha=0.5, label=\"noise floor C\")\n",
    "        try:\n",
    "            ax.set_xscale(\"log\", base=2)\n",
    "        except TypeError:\n",
    "            ax.set_xscale(\"log\", basex=2)\n",
    "        ax.set_yscale(\"log\")\n",
    "        ax.set_xlabel(\"Encode Dimension (z)\")\n",
    "        ax.set_ylabel(\"Weighted Prediction Error (MSE)\")\n",
    "        ax.set_title(f\"{env} — E(z)=A z^(−α)+C\")\n",
    "        ax.grid(True, which=\"both\", ls=\"--\", alpha=0.6)\n",
    "        ax.legend(fontsize=9)\n",
    "        fig.tight_layout()\n",
    "        p = os.path.join(out_dir, f\"{env}_ScalingLawFit.png\")\n",
    "        plt.savefig(p, dpi=300); plt.close(fig); print(f\"Saved scaling-law fit plot: {p}\")\n",
    "\n",
    "    if fits:\n",
    "        df_fits = pd.DataFrame(fits)\n",
    "        p = os.path.join(PROJECT_NAME, \"scaling_law_fits.csv\")\n",
    "        df_fits.to_csv(p, index=False)\n",
    "        print(f\"Saved scaling-law fit table: {p}\")\n",
    "        display(df_fits)\n",
    "    else:\n",
    "        print(\"[Skip] No scaling-law fits produced.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9c5ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use all points instead of mean"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "koopman",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9036fde0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved summary to Sep_21/evaluation_summary.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Environment</th>\n",
       "      <th>TrainSamples</th>\n",
       "      <th>EncodeDim</th>\n",
       "      <th>UseCovLoss</th>\n",
       "      <th>UseControlLoss</th>\n",
       "      <th>PredictionError</th>\n",
       "      <th>NormalizedCovariance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DampingPendulum</td>\n",
       "      <td>1000</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.014942</td>\n",
       "      <td>6.611577e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DampingPendulum</td>\n",
       "      <td>1000</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0.097864</td>\n",
       "      <td>2.747380e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DampingPendulum</td>\n",
       "      <td>1000</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.020307</td>\n",
       "      <td>2.094539e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DampingPendulum</td>\n",
       "      <td>1000</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>0.150161</td>\n",
       "      <td>1.223969e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DampingPendulum</td>\n",
       "      <td>1000</td>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.022343</td>\n",
       "      <td>1.270932e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>645</th>\n",
       "      <td>Polynomial</td>\n",
       "      <td>140000</td>\n",
       "      <td>12</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.000631</td>\n",
       "      <td>9.368436e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>646</th>\n",
       "      <td>Polynomial</td>\n",
       "      <td>140000</td>\n",
       "      <td>24</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.000469</td>\n",
       "      <td>4.363544e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>647</th>\n",
       "      <td>Polynomial</td>\n",
       "      <td>140000</td>\n",
       "      <td>24</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.000462</td>\n",
       "      <td>1.460223e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>648</th>\n",
       "      <td>Polynomial</td>\n",
       "      <td>140000</td>\n",
       "      <td>48</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.000444</td>\n",
       "      <td>2.988363e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>649</th>\n",
       "      <td>Polynomial</td>\n",
       "      <td>140000</td>\n",
       "      <td>48</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.000342</td>\n",
       "      <td>9.519075e-04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>650 rows √ó 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Environment  TrainSamples  EncodeDim  UseCovLoss  UseControlLoss  \\\n",
       "0    DampingPendulum          1000          2       False           False   \n",
       "1    DampingPendulum          1000          2       False            True   \n",
       "2    DampingPendulum          1000          2        True           False   \n",
       "3    DampingPendulum          1000          2        True            True   \n",
       "4    DampingPendulum          1000          4       False           False   \n",
       "..               ...           ...        ...         ...             ...   \n",
       "645       Polynomial        140000         12        True           False   \n",
       "646       Polynomial        140000         24       False           False   \n",
       "647       Polynomial        140000         24        True           False   \n",
       "648       Polynomial        140000         48       False           False   \n",
       "649       Polynomial        140000         48        True           False   \n",
       "\n",
       "     PredictionError  NormalizedCovariance  \n",
       "0           0.014942          6.611577e+05  \n",
       "1           0.097864          2.747380e+03  \n",
       "2           0.020307          2.094539e-07  \n",
       "3           0.150161          1.223969e-04  \n",
       "4           0.022343          1.270932e+04  \n",
       "..               ...                   ...  \n",
       "645         0.000631          9.368436e-08  \n",
       "646         0.000469          4.363544e+00  \n",
       "647         0.000462          1.460223e-03  \n",
       "648         0.000444          2.988363e+00  \n",
       "649         0.000342          9.519075e-04  \n",
       "\n",
       "[650 rows x 7 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üñºÔ∏è Saved plot: Sep_21/DampingPendulum/DampingPendulum_TrainSamples_vs_Error_LargestEncDim.png\n",
      "üñºÔ∏è Saved plot: Sep_21/DampingPendulum/DampingPendulum_EncodeDim_vs_Error_LargestTrainSamples.png\n",
      "üñºÔ∏è Saved plot: Sep_21/Franka/Franka_TrainSamples_vs_Error_LargestEncDim.png\n",
      "üñºÔ∏è Saved plot: Sep_21/Franka/Franka_EncodeDim_vs_Error_LargestTrainSamples.png\n",
      "üñºÔ∏è Saved plot: Sep_21/DoublePendulum/DoublePendulum_TrainSamples_vs_Error_LargestEncDim.png\n",
      "üñºÔ∏è Saved plot: Sep_21/DoublePendulum/DoublePendulum_EncodeDim_vs_Error_LargestTrainSamples.png\n",
      "üñºÔ∏è Saved plot: Sep_21/Polynomial/Polynomial_TrainSamples_vs_Error_LargestEncDim.png\n",
      "üñºÔ∏è Saved plot: Sep_21/Polynomial/Polynomial_EncodeDim_vs_Error_LargestTrainSamples.png\n",
      "üñºÔ∏è Saved plot: Sep_21/Kinova/Kinova_TrainSamples_vs_Error_LargestEncDim.png\n",
      "üñºÔ∏è Saved plot: Sep_21/Kinova/Kinova_EncodeDim_vs_Error_LargestTrainSamples.png\n",
      "üñºÔ∏è Saved plot: Sep_21/G1/G1_TrainSamples_vs_Error_LargestEncDim.png\n",
      "üñºÔ∏è Saved plot: Sep_21/G1/G1_EncodeDim_vs_Error_LargestTrainSamples.png\n",
      "üñºÔ∏è Saved plot: Sep_21/Go2/Go2_TrainSamples_vs_Error_LargestEncDim.png\n",
      "üñºÔ∏è Saved plot: Sep_21/Go2/Go2_EncodeDim_vs_Error_LargestTrainSamples.png\n",
      "üñºÔ∏è Saved combined plot: Sep_21/AllEnvs_RelError_vs_RelMultiplier_LargestTrainSamples.png\n",
      "üñºÔ∏è Saved scaling-law plot: Sep_21/DampingPendulum/DampingPendulum_ScalingLawFit_LargestTrainSamples.png\n",
      "üñºÔ∏è Saved scaling-law plot: Sep_21/Franka/Franka_ScalingLawFit_LargestTrainSamples.png\n",
      "üñºÔ∏è Saved scaling-law plot: Sep_21/DoublePendulum/DoublePendulum_ScalingLawFit_LargestTrainSamples.png\n",
      "üñºÔ∏è Saved scaling-law plot: Sep_21/Polynomial/Polynomial_ScalingLawFit_LargestTrainSamples.png\n",
      "üñºÔ∏è Saved scaling-law plot: Sep_21/Kinova/Kinova_ScalingLawFit_LargestTrainSamples.png\n",
      "üñºÔ∏è Saved scaling-law plot: Sep_21/G1/G1_ScalingLawFit_LargestTrainSamples.png\n",
      "üñºÔ∏è Saved scaling-law plot: Sep_21/Go2/Go2_ScalingLawFit_LargestTrainSamples.png\n",
      "‚úÖ Saved scaling-law fit table: Sep_21/scaling_law_fits_largest_train_samples.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Environment</th>\n",
       "      <th>A</th>\n",
       "      <th>alpha</th>\n",
       "      <th>C</th>\n",
       "      <th>R2_linear</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DampingPendulum</td>\n",
       "      <td>0.912926</td>\n",
       "      <td>2.793330</td>\n",
       "      <td>8.849887e-03</td>\n",
       "      <td>0.778184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Franka</td>\n",
       "      <td>0.000254</td>\n",
       "      <td>1.577690</td>\n",
       "      <td>9.740425e-08</td>\n",
       "      <td>0.830990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DoublePendulum</td>\n",
       "      <td>0.011816</td>\n",
       "      <td>2.170697</td>\n",
       "      <td>1.670130e-02</td>\n",
       "      <td>0.611817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Polynomial</td>\n",
       "      <td>0.000470</td>\n",
       "      <td>0.253914</td>\n",
       "      <td>2.753584e-04</td>\n",
       "      <td>0.330634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Kinova</td>\n",
       "      <td>3.516526</td>\n",
       "      <td>0.882522</td>\n",
       "      <td>1.275619e-19</td>\n",
       "      <td>0.967441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>G1</td>\n",
       "      <td>0.157902</td>\n",
       "      <td>0.491844</td>\n",
       "      <td>1.283856e-01</td>\n",
       "      <td>0.998491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Go2</td>\n",
       "      <td>0.319514</td>\n",
       "      <td>0.747842</td>\n",
       "      <td>2.051753e-01</td>\n",
       "      <td>0.999225</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Environment         A     alpha             C  R2_linear\n",
       "0  DampingPendulum  0.912926  2.793330  8.849887e-03   0.778184\n",
       "1           Franka  0.000254  1.577690  9.740425e-08   0.830990\n",
       "2   DoublePendulum  0.011816  2.170697  1.670130e-02   0.611817\n",
       "3       Polynomial  0.000470  0.253914  2.753584e-04   0.330634\n",
       "4           Kinova  3.516526  0.882522  1.275619e-19   0.967441\n",
       "5               G1  0.157902  0.491844  1.283856e-01   0.998491\n",
       "6              Go2  0.319514  0.747842  2.051753e-01   0.999225"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "from scipy.optimize import curve_fit\n",
    "\n",
    "# ============================\n",
    "# Configuration\n",
    "# ============================\n",
    "PROJECT_NAME = \"Sep_21\"\n",
    "\n",
    "ENVS          = [\"DampingPendulum\", \"Franka\", \"DoublePendulum\", \"Polynomial\", \"Kinova\", \"G1\", \"Go2\"]\n",
    "SEEDS         = [17382, 76849, 20965, 84902, 51194]\n",
    "TRAIN_SAMPLES = [1000, 4000, 16000, 64000, 140000]\n",
    "M_POLY        = 100\n",
    "\n",
    "U_DIM = {\"Franka\": 7, \"DoublePendulum\": 2, \"DampingPendulum\": 1, \"G1\": 23, \"Go2\": 12, \"Kinova\": 7}\n",
    "NORMALIZE = {\"G1\": \"norm\", \"Go2\": \"norm\"}\n",
    "REL_MULT_TARGETS = [1, 2, 4, 8, 16]\n",
    "\n",
    "# ============================\n",
    "# Helpers\n",
    "# ============================\n",
    "def gmean(vals, eps: float = 1e-12) -> float:\n",
    "    arr = np.asarray(list(vals), dtype=float)\n",
    "    arr = np.maximum(arr, eps)\n",
    "    return float(np.exp(np.mean(np.log(arr))))\n",
    "\n",
    "def env_has_control(env: str) -> bool:\n",
    "    return U_DIM.get(env, 0) > 0\n",
    "\n",
    "def find_dataset_path(env: str, m_val: int, ksteps: int = 15) -> str | None:\n",
    "    norm = NORMALIZE.get(env, \"nonorm\")\n",
    "    base_path = os.path.join(\"..\", \"data\", \"datasets\")\n",
    "    if env == \"Polynomial\":\n",
    "        path = os.path.join(base_path, f\"dataset_{env}_{norm}_m_{m_val}_Ktrain_140000_Kval_20000_Ktest_20000_Ksteps_1.pt\")\n",
    "    else:\n",
    "        path = os.path.join(base_path, f\"dataset_{env}_{norm}_Ktrain_140000_Kval_20000_Ktest_20000_Ksteps_{ksteps}.pt\")\n",
    "    return path if os.path.exists(path) else None\n",
    "\n",
    "# ============================\n",
    "# 1) Load and Validate CSV Log\n",
    "# ============================\n",
    "log_csv = os.path.join(\"..\", \"log\", PROJECT_NAME, \"koopman_results_log.csv\")\n",
    "assert os.path.exists(log_csv), f\"‚ùå CSV log not found: {log_csv}\"\n",
    "log = pd.read_csv(log_csv)\n",
    "\n",
    "required_cols = [\"env_name\", \"seed\", \"train_samples\", \"encode_dim\", \"test_Kloss\", \"test_CovLoss\", \"encode_dim_param\"]\n",
    "for col in required_cols:\n",
    "    assert col in log.columns, f\"‚ùå Log file is missing required column: '{col}'\"\n",
    "\n",
    "# ============================\n",
    "# 2) Infer State Dimension for Each Environment\n",
    "# ============================\n",
    "env_state_dim: dict[str, int | None] = {}\n",
    "for env in ENVS:\n",
    "    ds_path = find_dataset_path(env, M_POLY)\n",
    "    if ds_path is None:\n",
    "        env_state_dim[env] = None\n",
    "        continue\n",
    "    data = torch.load(ds_path, weights_only=False)\n",
    "    full_dim = int(data[\"Ktest_data\"].shape[2])\n",
    "    u = U_DIM.get(env, 0)\n",
    "    env_state_dim[env] = full_dim - u\n",
    "\n",
    "# ============================\n",
    "# 3) Filter Log for Relevant Experiments\n",
    "# ============================\n",
    "mask = (\n",
    "    log['env_name'].isin(ENVS) &\n",
    "    log['seed'].isin(SEEDS) &\n",
    "    log['train_samples'].isin(TRAIN_SAMPLES) &\n",
    "    log['encode_dim_param'].isin(REL_MULT_TARGETS) &\n",
    "    ((log['env_name'] != 'Polynomial') | (log['m'] == M_POLY))\n",
    ")\n",
    "filtered_log = log[mask].copy()\n",
    "\n",
    "if filtered_log.empty:\n",
    "    print(\"‚ùå No matching experiments found in the log after filtering. Please check your configuration.\")\n",
    "    raise SystemExit(0)\n",
    "\n",
    "# ============================\n",
    "# 4) Aggregate Results for Summary Table (df_agg)\n",
    "# ============================\n",
    "def normalize_cov(row):\n",
    "    zdim = row['encode_dim']\n",
    "    if zdim <= 1:\n",
    "        return row['test_CovLoss']\n",
    "    denominator = zdim * (zdim - 1)\n",
    "    return row['test_CovLoss'] / denominator\n",
    "\n",
    "filtered_log['NormalizedCovariance'] = filtered_log.apply(normalize_cov, axis=1)\n",
    "\n",
    "grouping_keys = [\n",
    "    \"env_name\", \"train_samples\", \"encode_dim\",\n",
    "    \"use_covariance_loss\", \"use_control_loss\"\n",
    "]\n",
    "df_agg = filtered_log.groupby(grouping_keys).agg(\n",
    "    PredictionError=(\"test_Kloss\", gmean),\n",
    "    NormalizedCovariance=(\"NormalizedCovariance\", gmean)\n",
    ").reset_index()\n",
    "\n",
    "df = df_agg.rename(columns={\n",
    "    \"env_name\": \"Environment\",\n",
    "    \"train_samples\": \"TrainSamples\",\n",
    "    \"encode_dim\": \"EncodeDim\",\n",
    "    \"use_covariance_loss\": \"UseCovLoss\",\n",
    "    \"use_control_loss\": \"UseControlLoss\"\n",
    "})\n",
    "\n",
    "# ============================\n",
    "# 5) Save Summary CSV\n",
    "# ============================\n",
    "os.makedirs(PROJECT_NAME, exist_ok=True)\n",
    "out_csv = os.path.join(PROJECT_NAME, \"evaluation_summary.csv\")\n",
    "df.to_csv(out_csv, index=False)\n",
    "print(f\"‚úÖ Saved summary to {out_csv}\")\n",
    "if not df.empty:\n",
    "    display(df)\n",
    "\n",
    "# ============================\n",
    "# 6) Per-Environment Plots\n",
    "# ============================\n",
    "if not df.empty:\n",
    "    for env in ENVS:\n",
    "        sub = df[(df.Environment == env) & (df.UseCovLoss == 1)]\n",
    "        if env_has_control(env) and \"UseControlLoss\" in df.columns:\n",
    "            sub = sub[sub.UseControlLoss == 1]\n",
    "        tag = \"(cov=1, ctrl=1)\" if env_has_control(env) else \"(cov=1)\"\n",
    "        if sub.empty: continue\n",
    "\n",
    "        out_dir = os.path.join(PROJECT_NAME, env); os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "        if not sub.empty and 'EncodeDim' in sub.columns:\n",
    "            max_encode_dim = sub['EncodeDim'].max()\n",
    "            sub_for_ts_plot = sub[sub['EncodeDim'] == max_encode_dim]\n",
    "            \n",
    "            gA = sub_for_ts_plot.groupby(\"TrainSamples\", as_index=False).agg(Error_gmean=(\"PredictionError\", gmean)).sort_values(\"TrainSamples\")\n",
    "            if not gA.empty:\n",
    "                fig, ax = plt.subplots(figsize=(8, 6))\n",
    "                ax.plot(gA[\"TrainSamples\"], gA[\"Error_gmean\"], marker=\"o\")\n",
    "                title = f\"{env} ‚Äî TrainSamples vs Error\\n(at largest EncodeDim: {max_encode_dim}) {tag}\"\n",
    "                ax.set(yscale=\"log\", xlabel=\"Train Samples (Ktrain)\", ylabel=\"Prediction Error (MSE, geom. mean)\", title=title)\n",
    "                ax.grid(True, which=\"both\", ls=\"--\", alpha=0.6); fig.tight_layout()\n",
    "                p = os.path.join(out_dir, f\"{env}_TrainSamples_vs_Error_LargestEncDim.png\"); plt.savefig(p, dpi=300); plt.close(fig)\n",
    "                print(f\"üñºÔ∏è Saved plot: {p}\")\n",
    "\n",
    "        if not sub.empty and 'TrainSamples' in sub.columns:\n",
    "            max_train_samples = sub['TrainSamples'].max()\n",
    "            sub_for_ed_plot = sub[sub['TrainSamples'] == max_train_samples]\n",
    "\n",
    "            gB = sub_for_ed_plot.groupby(\"EncodeDim\", as_index=False).agg(Error_gmean=(\"PredictionError\", gmean)).sort_values(\"EncodeDim\")\n",
    "            if not gB.empty:\n",
    "                fig, ax = plt.subplots(figsize=(8, 6))\n",
    "                ax.plot(gB[\"EncodeDim\"], gB[\"Error_gmean\"], marker=\"s\")\n",
    "                title = f\"{env} ‚Äî EncodeDim vs Error\\n(at largest TrainSamples: {max_train_samples}) {tag}\"\n",
    "                ax.set(xscale=\"linear\", yscale=\"log\", xlabel=\"Encode Dimension (z)\", ylabel=\"Prediction Error (MSE, geom. mean)\", title=title)\n",
    "                ax.grid(True, which=\"both\", ls=\"--\", alpha=0.6); fig.tight_layout()\n",
    "                p = os.path.join(out_dir, f\"{env}_EncodeDim_vs_Error_LargestTrainSamples.png\"); plt.savefig(p, dpi=300); plt.close(fig)\n",
    "                print(f\"üñºÔ∏è Saved plot: {p}\")\n",
    "\n",
    "\n",
    "# ============================\n",
    "# 7) Combined Plot: Normalized Error vs. Relative Multiplier\n",
    "# ============================\n",
    "# **MODIFIED**: This section now filters for the largest training sample size.\n",
    "if not df.empty:\n",
    "    def nearest_rel_mult(env: str, z_abs: float) -> float:\n",
    "        st = env_state_dim.get(env, None)\n",
    "        if not st or st <= 0: return float(\"nan\")\n",
    "        r = float(z_abs) / float(st)\n",
    "        return float(min(REL_MULT_TARGETS, key=lambda m: abs(m - r)))\n",
    "\n",
    "    rows = []\n",
    "    for env in ENVS:\n",
    "        sub = df[(df.Environment == env) & (df.UseCovLoss == 1)]\n",
    "        if env_has_control(env) and \"UseControlLoss\" in df.columns:\n",
    "            sub = sub[sub.UseControlLoss == 1]\n",
    "        if sub.empty: continue\n",
    "\n",
    "        # **NEW**: Filter the subset to only include runs with the largest sample size\n",
    "        max_train_samples = sub['TrainSamples'].max()\n",
    "        sub = sub[sub['TrainSamples'] == max_train_samples]\n",
    "\n",
    "        sub = sub.copy()\n",
    "        sub[\"RelMult\"] = sub[\"EncodeDim\"].apply(lambda z: nearest_rel_mult(env, z))\n",
    "        sub = sub.replace([np.inf, -np.inf], np.nan).dropna(subset=[\"RelMult\"])\n",
    "        \n",
    "        # Now, groupby operates on the data for the largest training set\n",
    "        g = sub.groupby(\"RelMult\", as_index=False).agg(PredictionError_gmean=(\"PredictionError\", gmean)).sort_values(\"RelMult\")\n",
    "        if g.empty: continue\n",
    "\n",
    "        E0 = g.loc[g[\"RelMult\"].idxmin(), \"PredictionError_gmean\"]\n",
    "        g[\"RelError\"] = g[\"PredictionError_gmean\"] / max(E0, 1e-12)\n",
    "        if g.shape[0] >= 2:\n",
    "            x = np.log(g[\"RelMult\"].to_numpy(dtype=float)); y = np.log(np.maximum(g[\"RelError\"].to_numpy(dtype=float), 1e-12))\n",
    "            b1, _ = np.polyfit(x, y, 1); g[\"Slope\"] = float(b1)\n",
    "        else:\n",
    "            g[\"Slope\"] = float(\"nan\")\n",
    "\n",
    "        k = min(2, len(g)); g[\"NoiseRel\"] = float(g.tail(k)[\"RelError\"].mean())\n",
    "        rows.append(g.assign(Environment=env))\n",
    "\n",
    "    if rows:\n",
    "        GG = pd.concat(rows, ignore_index=True)\n",
    "        fig, ax = plt.subplots(figsize=(8, 6))\n",
    "        for env in sorted(GG[\"Environment\"].unique()):\n",
    "            ge = GG[GG.Environment == env].sort_values(\"RelMult\")\n",
    "            (line,) = ax.plot(ge[\"RelMult\"], ge[\"RelError\"], marker=\"o\", label=f\"{env} (slope={ge['Slope'].iloc[0]:.2f})\")\n",
    "            ax.hlines(ge[\"NoiseRel\"].iloc[0], ge[\"RelMult\"].min(), ge[\"RelMult\"].max(), linestyles=\"dashed\", alpha=0.4, colors=[line.get_color()])\n",
    "        try:\n",
    "            ax.set_xscale(\"log\", base=2)\n",
    "        except TypeError:\n",
    "            ax.set_xscale(\"log\", basex=2)\n",
    "            \n",
    "        title = \"Normalized Error vs. Relative Multiplier (Largest TrainSamples)\"\n",
    "        ax.set(xlabel=\"Relative encode multiplier (z / state_dim, log‚ÇÇ)\", ylabel=\"Relative prediction error (E / E@min multiplier)\", title=title)\n",
    "        ax.grid(True, which=\"both\", ls=\"--\", alpha=0.6); ax.legend(ncol=2, fontsize=9); fig.tight_layout()\n",
    "        p = os.path.join(PROJECT_NAME, \"AllEnvs_RelError_vs_RelMultiplier_LargestTrainSamples.png\"); plt.savefig(p, dpi=300); plt.close(fig)\n",
    "        print(f\"üñºÔ∏è Saved combined plot: {p}\")\n",
    "\n",
    "# ============================\n",
    "# 8) Scaling-Law Fits per Environment\n",
    "# ============================\n",
    "if not filtered_log.empty:\n",
    "    def scaling_model(D, A, alpha, C):\n",
    "        return A * np.power(D, -alpha) + C\n",
    "\n",
    "    fit_results = []\n",
    "    for env in ENVS:\n",
    "        sub = filtered_log[(filtered_log.env_name == env) & (filtered_log.use_covariance_loss == 1)]\n",
    "        if env_has_control(env) and \"use_control_loss\" in filtered_log.columns:\n",
    "            sub = sub[sub.use_control_loss == 1]\n",
    "        \n",
    "        if sub.empty:\n",
    "            continue\n",
    "\n",
    "        max_train_samples = sub['train_samples'].max()\n",
    "        sub = sub[sub['train_samples'] == max_train_samples]\n",
    "        \n",
    "        if sub['encode_dim'].nunique() < 3:\n",
    "            print(f\"‚ö†Ô∏è Skipping scaling-law for {env}: not enough unique encode_dim points at largest sample size ({max_train_samples}).\")\n",
    "            continue\n",
    "\n",
    "        D, E = sub[\"encode_dim\"].astype(float).to_numpy(), sub[\"test_Kloss\"].astype(float).to_numpy()\n",
    "\n",
    "        try:\n",
    "            popt, pcov = curve_fit(scaling_model, D, E, p0=[E.max(), 0.7, E.min()],\n",
    "                                   bounds=([0.0, 0.0, 0.0], [np.inf, 4.0, np.inf]), maxfev=20000)\n",
    "            A_hat, alpha_hat, C_hat = popt\n",
    "            E_pred = scaling_model(D, A_hat, alpha_hat, C_hat)\n",
    "            ss_res = np.sum((E - E_pred) ** 2)\n",
    "            ss_tot = np.sum((E - np.mean(E)) ** 2)\n",
    "            R2_lin = 1.0 - ss_res / ss_tot if ss_tot > 0 else float(\"nan\")\n",
    "\n",
    "            fit_results.append({\"Environment\": env, \"A\": A_hat, \"alpha\": alpha_hat, \"C\": C_hat, \"R2_linear\": R2_lin})\n",
    "\n",
    "            out_dir = os.path.join(PROJECT_NAME, env); os.makedirs(out_dir, exist_ok=True)\n",
    "            fig, ax = plt.subplots(figsize=(7, 5))\n",
    "\n",
    "            ax.plot(D, E, \"o\", label=\"Data (all seeds)\", alpha=0.5)\n",
    "\n",
    "            D_fit = np.geomspace(D.min(), D.max(), 256)\n",
    "            E_fit = scaling_model(D_fit, A_hat, alpha_hat, C_hat)\n",
    "            ax.plot(D_fit, E_fit, \"-\", color='red', linewidth=2, label=f\"Fit: A={A_hat:.3g}, Œ±={alpha_hat:.2f}, C={C_hat:.3g} (R¬≤={R2_lin:.2f})\")\n",
    "            \n",
    "            try:\n",
    "                ax.set_xscale(\"log\", base=2)\n",
    "            except TypeError:\n",
    "                ax.set_xscale(\"log\", basex=2)\n",
    "\n",
    "            title = f\"{env} ‚Äî Scaling Law Fit (Largest TrainSamples: {max_train_samples})\"\n",
    "            ax.set(yscale=\"log\", xlabel=\"Encode Dimension (z)\", ylabel=\"Prediction Error (MSE)\", title=title)\n",
    "            ax.grid(True, which=\"both\", ls=\"--\", alpha=0.6); ax.legend(fontsize=9); fig.tight_layout()\n",
    "            p = os.path.join(out_dir, f\"{env}_ScalingLawFit_LargestTrainSamples.png\"); plt.savefig(p, dpi=300); plt.close(fig)\n",
    "            print(f\"üñºÔ∏è Saved scaling-law plot: {p}\")\n",
    "\n",
    "        except Exception as ex:\n",
    "            print(f\"‚ö†Ô∏è Could not fit scaling law for {env}: {ex}\")\n",
    "\n",
    "    if fit_results:\n",
    "        df_fits = pd.DataFrame(fit_results)\n",
    "        p = os.path.join(PROJECT_NAME, \"scaling_law_fits_largest_train_samples.csv\"); df_fits.to_csv(p, index=False)\n",
    "        print(f\"‚úÖ Saved scaling-law fit table: {p}\")\n",
    "        display(df_fits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1daf521e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PredictionError</th>\n",
       "      <th>NormalizedCovariance</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TrainSamples</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>0.031757</td>\n",
       "      <td>57606.626630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4000</th>\n",
       "      <td>0.022341</td>\n",
       "      <td>29880.345943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16000</th>\n",
       "      <td>0.020648</td>\n",
       "      <td>29513.165659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64000</th>\n",
       "      <td>0.021046</td>\n",
       "      <td>31029.251127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140000</th>\n",
       "      <td>0.019740</td>\n",
       "      <td>3989.715538</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              PredictionError  NormalizedCovariance\n",
       "TrainSamples                                       \n",
       "1000                 0.031757          57606.626630\n",
       "4000                 0.022341          29880.345943\n",
       "16000                0.020648          29513.165659\n",
       "64000                0.021046          31029.251127\n",
       "140000               0.019740           3989.715538"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['Environment']=='DampingPendulum'][['TrainSamples', 'PredictionError', 'NormalizedCovariance']].groupby(\"TrainSamples\").mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a07637c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "koopman",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and setup\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from torch import nn\n",
    "import sys\n",
    "\n",
    "# Add parent directories to sys.path so your modules can be imported\n",
    "sys.path.append(\"../scripts\")\n",
    "sys.path.append(\"../utility\")\n",
    "\n",
    "# Import your custom modules\n",
    "from network import KoopmanNet\n",
    "from dataset import KoopmanDatasetCollector\n",
    "\n",
    "# Set parameters here instead of using argparse\n",
    "project_name = \"Koopman_Results_Apr_8_2\"   # Project name used during training\n",
    "gamma = 0.8             # Gamma discount factor (e.g., 0.8 as in training)\n",
    "\n",
    "# Create the main folder for figures if it doesn't exist\n",
    "if not os.path.exists(project_name):\n",
    "    os.makedirs(project_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, data, u_dim, gamma, state_dim, device):\n",
    "    \"\"\"\n",
    "    Given a model and test data, compute:\n",
    "      - The weighted (gamma-decayed) multi-step prediction error (aggregated MSE),\n",
    "      - The unweighted error at each prediction step (for plotting loss curves),\n",
    "      - The normalized covariance loss on the learned encoding.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        steps = data.shape[0]\n",
    "        # Initialize the encoding from the first time step:\n",
    "        if u_dim is None:\n",
    "            X = model.encode(data[0].to(device))\n",
    "        else:\n",
    "            # For systems with control inputs, the network encodes the state portion only.\n",
    "            X = model.encode(data[0, :, u_dim:].to(device))\n",
    "            \n",
    "        # Save initial encoding for covariance evaluation\n",
    "        encoded_initial = X[:, state_dim:]\n",
    "        \n",
    "        weighted_loss = 0.0\n",
    "        beta = 1.0\n",
    "        beta_sum = 0.0\n",
    "        step_errors = []\n",
    "        # Iterate through prediction steps to compute error values\n",
    "        for i in range(steps - 1):\n",
    "            if u_dim is None:\n",
    "                X = model.forward(X, None)\n",
    "                target = data[i+1].to(device)\n",
    "            else:\n",
    "                X = model.forward(X, data[i, :, :u_dim].to(device))\n",
    "                target = data[i+1, :, u_dim:].to(device)\n",
    "            error = nn.MSELoss()(X[:, :state_dim], target)\n",
    "            step_errors.append(error.item())\n",
    "            weighted_loss += beta * error\n",
    "            beta_sum += beta\n",
    "            beta *= gamma\n",
    "        weighted_loss /= beta_sum\n",
    "\n",
    "        # Compute covariance loss on the encoded representation:\n",
    "        z = encoded_initial  # shape: (num_trajectories, encode_dim)\n",
    "        z_mean = torch.mean(z, dim=0, keepdim=True)\n",
    "        z_centered = z - z_mean\n",
    "        cov_matrix = (z_centered.t() @ z_centered) / (z_centered.size(0) - 1)\n",
    "        diag_cov = torch.diag(torch.diag(cov_matrix))\n",
    "        off_diag = cov_matrix - diag_cov\n",
    "        cov_loss_val = torch.norm(off_diag, p='fro')**2\n",
    "        encode_dim = X.shape[1] - state_dim\n",
    "        normalized_cov_loss = (cov_loss_val.item() / (encode_dim * (encode_dim - 1))\n",
    "                               if encode_dim > 1 else cov_loss_val.item())\n",
    "    return weighted_loss.item(), step_errors, normalized_cov_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary Table:\n",
      "    Environment  EncodeDim  CovReg  WeightedError  NormalizedCovLoss\n",
      "0   LogisticMap          4       0       0.404513           0.400132\n",
      "1   LogisticMap        256       0       0.400263           0.000586\n",
      "2   LogisticMap       1024       0       0.412784           0.003377\n",
      "3   LogisticMap        256       1       0.399289           0.000039\n",
      "4   LogisticMap          4       1       0.406822           0.000340\n",
      "..          ...        ...     ...            ...                ...\n",
      "65          Go2          4       1       0.192814           0.000002\n",
      "66          Go2         16       0       0.187583           0.036239\n",
      "67          Go2        256       0       0.161987           0.000250\n",
      "68          Go2       1024       1       0.153294           0.000009\n",
      "69          Go2        256       1       0.161545           0.000046\n",
      "\n",
      "[70 rows x 5 columns]\n",
      "Summary table saved to evaluation_summary.csv\n"
     ]
    }
   ],
   "source": [
    "# Main evaluation\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define experimental parameters\n",
    "envs = ['LogisticMap', 'DampingPendulum', 'Franka', 'DoublePendulum', 'Polynomial', 'G1', 'Go2']\n",
    "encode_dims = [4, 16, 64, 256, 1024]\n",
    "cov_regs = [0, 1]\n",
    "seeds = [1, 2, 3, 4, 5]\n",
    "\n",
    "# Directory containing best-model checkpoints (assumes training saved them in this naming format)\n",
    "best_models_dir = os.path.join(\"..\", \"log\", \"best_models\", project_name)\n",
    "results = {}  # dictionary keyed by (env, encode_dim, cov_reg)\n",
    "\n",
    "# The checkpoint file naming convention: \n",
    "# \"best_model_norm_{env}_{encode_dim}_{cov_reg}_{seed}.pth\"\n",
    "pattern = r\"best_model_norm_(\\w+)_(\\d+)_(\\d+)_(\\d+)\\.pth\"\n",
    "all_files = glob.glob(os.path.join(best_models_dir, \"best_model_norm_*.pth\"))\n",
    "for f in all_files:\n",
    "    basename = os.path.basename(f)\n",
    "    m = re.match(pattern, basename)\n",
    "    if m:\n",
    "        env_name = m.group(1)\n",
    "        enc_dim = int(m.group(2))\n",
    "        cov_reg_val = int(m.group(3))\n",
    "        seed_val = int(m.group(4))\n",
    "        # Skip the configuration (encode_dim==1 and cov_reg==1) as in training\n",
    "        if enc_dim == 1 and cov_reg_val == 1:\n",
    "            continue\n",
    "        if enc_dim not in encode_dims:\n",
    "            continue\n",
    "        key = (env_name, enc_dim, cov_reg_val)\n",
    "        if key not in results:\n",
    "            results[key] = []\n",
    "        results[key].append((seed_val, f))\n",
    "        \n",
    "# Prepare storage for summary table and step error curves (for curve plots)\n",
    "summary = []        # List of dicts for table rows\n",
    "step_error_curves = {}  # Key: (env, encode_dim, cov_reg) -> averaged step error list\n",
    "\n",
    "# Loop over environments and evaluate configurations\n",
    "for env in envs:\n",
    "    # Set Ksteps (prediction horizon): for \"Polynomial\" and \"LogisticMap\" use horizon 1, otherwise 10.\n",
    "    Ksteps = 1 if env in [\"Polynomial\", \"LogisticMap\"] else 10\n",
    "    \n",
    "    # Load the test dataset (assumes same naming convention as in train.py)\n",
    "    norm_str = \"norm\"  # because normalize=True was used during training\n",
    "    dataset_path = os.path.join(\"..\", \"data\", \"datasets\", \n",
    "                                f\"dataset_{env}_{norm_str}_Ktrain_60000_Kval_20000_Ktest_20000_Ksteps_{Ksteps}.pt\")\n",
    "    if not os.path.exists(dataset_path):\n",
    "        print(f\"Dataset file {dataset_path} not found for environment {env}, skipping.\")\n",
    "        continue\n",
    "    data_dict = torch.load(dataset_path, weights_only=False)\n",
    "    # Convert test data to tensor if necessary\n",
    "    test_data = torch.from_numpy(data_dict[\"Ktest_data\"]).float().to(device)\n",
    "\n",
    "    # Determine state and control dimensions:\n",
    "    if env in [\"Franka\", \"DoublePendulum\", \"DampingPendulum\", \"G1\", \"Go2\"]:\n",
    "        if env == \"Franka\":\n",
    "            u_dim = 7\n",
    "            state_dim = test_data.shape[2] - u_dim\n",
    "        elif env == \"DoublePendulum\":\n",
    "            u_dim = 2\n",
    "            state_dim = test_data.shape[2] - u_dim\n",
    "        elif env == \"DampingPendulum\":\n",
    "            u_dim = 1\n",
    "            state_dim = test_data.shape[2] - u_dim\n",
    "        elif env == \"G1\":\n",
    "            u_dim = 37\n",
    "            state_dim = test_data.shape[2] - u_dim\n",
    "        elif env == \"Go2\":\n",
    "            u_dim = 12\n",
    "            state_dim = test_data.shape[2] - u_dim\n",
    "        else:\n",
    "            u_dim = None\n",
    "            state_dim = test_data.shape[2]\n",
    "    else:\n",
    "        u_dim = None\n",
    "        state_dim = test_data.shape[2]\n",
    "\n",
    "    # Loop over each configuration for the current environment:\n",
    "    for key in results:\n",
    "        if key[0] != env:\n",
    "            continue\n",
    "        enc_dim, cov_reg_val = key[1], key[2]\n",
    "        weighted_errors = []\n",
    "        norm_cov_losses = []\n",
    "        all_step_errors = []\n",
    "        # Evaluate each seed run\n",
    "        for (seed_val, filepath) in results[key]:\n",
    "            checkpoint = torch.load(filepath, map_location=device)\n",
    "            # Reconstruct network architecture:\n",
    "            layers = checkpoint['layer']\n",
    "            Nkoopman = state_dim + enc_dim\n",
    "            model = KoopmanNet(layers, Nkoopman, u_dim)\n",
    "            model.load_state_dict(checkpoint['model'])\n",
    "            model.to(device)\n",
    "            # Evaluate on test data\n",
    "            weighted_err, step_errs, norm_cov_loss = evaluate_model(model, test_data, u_dim, gamma, state_dim, device)\n",
    "            weighted_errors.append(weighted_err)\n",
    "            norm_cov_losses.append(norm_cov_loss)\n",
    "            all_step_errors.append(step_errs)\n",
    "        if len(weighted_errors) == 0:\n",
    "            continue\n",
    "        avg_weighted_err = np.mean(weighted_errors)\n",
    "        avg_norm_cov_loss = np.mean(norm_cov_losses)\n",
    "        avg_step_errors = np.mean(all_step_errors, axis=0)\n",
    "        summary.append({\n",
    "            \"Environment\": env,\n",
    "            \"EncodeDim\": enc_dim,\n",
    "            \"CovReg\": cov_reg_val,\n",
    "            \"WeightedError\": avg_weighted_err,\n",
    "            \"NormalizedCovLoss\": avg_norm_cov_loss\n",
    "        })\n",
    "        step_error_curves[(env, enc_dim, cov_reg_val)] = avg_step_errors\n",
    "\n",
    "# Create a summary table (using pandas) and save it as CSV.\n",
    "df = pd.DataFrame(summary)\n",
    "print(\"Summary Table:\")\n",
    "print(df)\n",
    "table_csv_path = \"evaluation_summary.csv\"\n",
    "df.to_csv(table_csv_path, index=False)\n",
    "print(f\"Summary table saved to {table_csv_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved plot: Koopman_Results_Apr_8_2/LogisticMap/MultiStepError_LogisticMap.png\n",
      "Saved plot: Koopman_Results_Apr_8_2/DampingPendulum/MultiStepError_DampingPendulum.png\n",
      "Saved plot: Koopman_Results_Apr_8_2/Franka/MultiStepError_Franka.png\n",
      "Saved plot: Koopman_Results_Apr_8_2/DoublePendulum/MultiStepError_DoublePendulum.png\n",
      "Saved plot: Koopman_Results_Apr_8_2/Polynomial/MultiStepError_Polynomial.png\n",
      "Saved plot: Koopman_Results_Apr_8_2/G1/MultiStepError_G1.png\n",
      "Saved plot: Koopman_Results_Apr_8_2/Go2/MultiStepError_Go2.png\n",
      "Saved plot: Koopman_Results_Apr_8_2/LogisticMap/NormalizedCovLoss_LogisticMap.png\n",
      "Saved plot: Koopman_Results_Apr_8_2/DampingPendulum/NormalizedCovLoss_DampingPendulum.png\n",
      "Saved plot: Koopman_Results_Apr_8_2/Franka/NormalizedCovLoss_Franka.png\n",
      "Saved plot: Koopman_Results_Apr_8_2/DoublePendulum/NormalizedCovLoss_DoublePendulum.png\n",
      "Saved plot: Koopman_Results_Apr_8_2/Polynomial/NormalizedCovLoss_Polynomial.png\n",
      "Saved plot: Koopman_Results_Apr_8_2/G1/NormalizedCovLoss_G1.png\n",
      "Saved plot: Koopman_Results_Apr_8_2/Go2/NormalizedCovLoss_Go2.png\n",
      "Saved plot: Koopman_Results_Apr_8_2/LogisticMap/MultiStepLossCurves_LogisticMap.png\n",
      "Saved plot: Koopman_Results_Apr_8_2/DampingPendulum/MultiStepLossCurves_DampingPendulum.png\n",
      "Saved plot: Koopman_Results_Apr_8_2/Franka/MultiStepLossCurves_Franka.png\n",
      "Saved plot: Koopman_Results_Apr_8_2/DoublePendulum/MultiStepLossCurves_DoublePendulum.png\n",
      "Saved plot: Koopman_Results_Apr_8_2/Polynomial/MultiStepLossCurves_Polynomial.png\n",
      "Saved plot: Koopman_Results_Apr_8_2/G1/MultiStepLossCurves_G1.png\n",
      "Saved plot: Koopman_Results_Apr_8_2/Go2/MultiStepLossCurves_Go2.png\n"
     ]
    }
   ],
   "source": [
    "# Plot 1: Average Multi-step Prediction Error vs. Encode Dimension\n",
    "\n",
    "for env in envs:\n",
    "    df_env = df[df[\"Environment\"] == env]\n",
    "    if df_env.empty:\n",
    "        continue\n",
    "    \n",
    "    # Create output directory for this environment\n",
    "    out_dir = os.path.join(project_name, env)\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    for cov_reg_val in cov_regs:\n",
    "        df_subset = df_env[df_env[\"CovReg\"] == cov_reg_val]\n",
    "        if df_subset.empty:\n",
    "            continue\n",
    "        df_subset = df_subset.sort_values(by=\"EncodeDim\")\n",
    "        x = df_subset[\"EncodeDim\"].values\n",
    "        y = df_subset[\"WeightedError\"].values\n",
    "        label = f\"CovReg {'on' if cov_reg_val == 1 else 'off'}\"\n",
    "        plt.plot(x, y, marker='o', label=label)\n",
    "    plt.xscale('log')\n",
    "    plt.xlabel(\"Encode Dimension (log scale)\")\n",
    "    plt.ylabel(\"Average Multi-step Prediction Error (MSE)\")\n",
    "    plt.yscale('log')\n",
    "\n",
    "    plt.title(f\"Multi-step Prediction Error vs. Encode Dimension for {env}\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, which=\"both\", ls=\"--\")\n",
    "    plt.tight_layout()\n",
    "    fig_path = os.path.join(out_dir, f\"MultiStepError_{env}.png\")\n",
    "    plt.savefig(fig_path, dpi=300)\n",
    "    print(f\"Saved plot: {fig_path}\")\n",
    "    plt.close()\n",
    "\n",
    "# Plot 2: Normalized Covariance Loss vs. Encode Dimension\n",
    "\n",
    "for env in envs:\n",
    "    df_env = df[df[\"Environment\"] == env]\n",
    "    if df_env.empty:\n",
    "        continue\n",
    "    \n",
    "    out_dir = os.path.join(project_name, env)\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    for cov_reg_val in cov_regs:\n",
    "        df_subset = df_env[df_env[\"CovReg\"] == cov_reg_val]\n",
    "        if df_subset.empty:\n",
    "            continue\n",
    "        df_subset = df_subset.sort_values(by=\"EncodeDim\")\n",
    "        x = df_subset[\"EncodeDim\"].values\n",
    "        y = df_subset[\"NormalizedCovLoss\"].values\n",
    "        label = f\"CovReg {'on' if cov_reg_val == 1 else 'off'}\"\n",
    "        plt.plot(x, y, marker='o', label=label)\n",
    "    plt.xscale('log')\n",
    "    plt.xlabel(\"Encode Dimension (log scale)\")\n",
    "    plt.ylabel(\"Normalized Covariance Loss\")\n",
    "    plt.title(f\"Normalized Covariance Loss vs. Encode Dimension for {env}\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, which=\"both\", ls=\"--\")\n",
    "    plt.tight_layout()\n",
    "    fig_path = os.path.join(out_dir, f\"NormalizedCovLoss_{env}.png\")\n",
    "    plt.savefig(fig_path, dpi=300)\n",
    "    print(f\"Saved plot: {fig_path}\")\n",
    "    plt.close()\n",
    "\n",
    "# Plot 3: Multi-step Loss Curves for each environment\n",
    "\n",
    "for env in envs:\n",
    "    out_dir = os.path.join(project_name, env)\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    for cov_reg_val in cov_regs:\n",
    "        for enc_dim in encode_dims:\n",
    "            key = (env, enc_dim, cov_reg_val)\n",
    "            if key in step_error_curves:\n",
    "                step_errs = step_error_curves[key]\n",
    "                steps_range = np.arange(1, len(step_errs) + 1)\n",
    "                label = f\"Enc: {enc_dim}, CovReg: {'on' if cov_reg_val == 1 else 'off'}\"\n",
    "                plt.plot(steps_range, step_errs, marker='o', label=label)\n",
    "    plt.xlabel(\"Prediction Step\")\n",
    "    plt.ylabel(\"MSE Loss at Step\")\n",
    "    plt.title(f\"Multi-step Loss Curves for {env}\")\n",
    "    plt.legend(fontsize='small', ncol=2)\n",
    "    plt.grid(True, ls=\"--\")\n",
    "    plt.tight_layout()\n",
    "    fig_path = os.path.join(out_dir, f\"MultiStepLossCurves_{env}.png\")\n",
    "    plt.savefig(fig_path, dpi=300)\n",
    "    print(f\"Saved plot: {fig_path}\")\n",
    "    plt.close()\n",
    "    \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "koopman",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
